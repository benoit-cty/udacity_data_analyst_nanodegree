{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Wrangling project\n",
    "\n",
    "Your tasks in this project are as follows:\n",
    "\n",
    "Data wrangling, which consists of:\n",
    "- Gathering data\n",
    "- Assessing data\n",
    "- Cleaning data\n",
    "- Storing, analyzing, and visualizing your wrangled data\n",
    "\n",
    "Reporting on :\n",
    "1. your data wrangling efforts\n",
    "2. your data analyses and visualizations\n",
    "\n",
    "Gathering Data for this Project\n",
    "\n",
    "Gather each of the three pieces of data as described below in a Jupyter Notebook titled wrangle_act.ipynb:\n",
    "\n",
    "The WeRateDogs Twitter archive. I am giving this file to you, so imagine it as a file on hand. Download this file manually.\n",
    "\n",
    "The tweet image predictions, i.e., what breed of dog (or other object, animal, etc.) is present in each tweet according to a neural network. This file (image_predictions.tsv) is hosted on Udacity's servers and should be downloaded programmatically using the Requests library and the following URL: https://d17h27t6h515a5.cloudfront.net/topher/2017/August/599fd2ad_image-predictions/image-predictions.tsv\n",
    "\n",
    "Each tweet's retweet count and favorite (\"like\") count at minimum, and any additional data you find interesting. Using the tweet IDs in the WeRateDogs Twitter archive, query the Twitter API for each tweet's JSON data using Python's Tweepy library and store each tweet's entire set of JSON data in a file called tweet_json.txt file. Each tweet's JSON data should be written to its own line. Then read this .txt file line by line into a pandas DataFrame with (at minimum) tweet ID, retweet count, and favorite count.\n",
    "\n",
    "**Note: do not include your Twitter API keys, secrets, and tokens in your project submission.**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Put these at the top of every notebook, to get automatic reloading and inline plotting\n",
    "%reload_ext autoreload\n",
    "%autoreload 2\n",
    "%matplotlib inline\n",
    "from IPython.core.interactiveshell import InteractiveShell\n",
    "InteractiveShell.ast_node_interactivity = \"all\"\n",
    "def pv(variable):\n",
    "    if(len(variable)>1): print(variable, \":\", eval(variable))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install an extension to collapse cell to have a better view on the entire notebook\n",
    "#!pip install jupyter_contrib_nbextensions\n",
    "#!jupyter contrib nbextension install --user"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Read The WeRateDogs Twitter archive\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "'file' n’est pas reconnu en tant que commande interne\n",
      "ou externe, un programme exécutable ou un fichier de commandes.\n",
      "'head' n’est pas reconnu en tant que commande interne\n",
      "ou externe, un programme exécutable ou un fichier de commandes.\n",
      "'tail' n’est pas reconnu en tant que commande interne\n",
      "ou externe, un programme exécutable ou un fichier de commandes.\n"
     ]
    }
   ],
   "source": [
    "!file twitter-archive-enhanced.csv\n",
    "!head -c200 twitter-archive-enhanced.csv\n",
    "!tail -c200 twitter-archive-enhanced.csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "df_twitter_archive_enhanced = pd.read_csv('twitter-archive-enhanced.csv')\n",
    "# df_twitter_archive_enhanced.head(3)\n",
    "# df_twitter_archive_enhanced.shape\n",
    "# df_twitter_archive_enhanced.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Read Image prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "335079"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "File retreived\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "import os\n",
    "folder_name = \"./\"\n",
    "url = \"https://d17h27t6h515a5.cloudfront.net/topher/2017/August/599fd2ad_image-predictions/image-predictions.tsv\"\n",
    "pred_filename = url.split('/')[-1]\n",
    "response = requests.get(url)\n",
    "with open(os.path.join(folder_name, pred_filename), mode='wb') as file:\n",
    "    file.write(response.content)\n",
    "print(\"File retreived\")\n",
    "import pandas as pd\n",
    "df_pred = pd.read_csv(folder_name + pred_filename, sep=\"\\t\")\n",
    "#df_pred.head(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Query Twitter API for each tweet in the Twitter archive and save JSON in a text file\n",
    "We will call the Twitter API for every Tweet we have in df_twitter_archive_enhanced.tweet_id and add the information to our existing Pandas dataset.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Installing Tweepy'"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''Installing Tweepy'''\n",
    "#!pip install tweepy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Create a config file to store the credentials.'"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''Create a config file to store the credentials.'''\n",
    "# import configparser\n",
    "# config = configparser.ConfigParser()\n",
    "# config['TWITTER'] = {'consumer_key': '45',\n",
    "#                     'consumer_secret': 'yes',\n",
    "#                     'access_token': 'yes',\n",
    "#                     'access_secret': '9'}\n",
    "# with open('config.ini', 'w') as configfile:\n",
    "#     config.write(configfile)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"It's taking a long time, don't do it again !\""
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-8-269870962ea0>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      8\u001b[0m \u001b[0mtweets_filename\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;34m\"tweet_json-bco.txt\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      9\u001b[0m \u001b[1;34m'''It's taking a long time, don't do it again !'''\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 10\u001b[1;33m \u001b[1;32mraise\u001b[0m \u001b[0mKeyboardInterrupt\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     11\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     12\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import tweepy\n",
    "from tweepy import OAuthHandler\n",
    "import json\n",
    "from timeit import default_timer as timer\n",
    "from tqdm import tqdm # For progress bar\n",
    "import configparser\n",
    "\n",
    "tweets_filename = \"tweet_json-bco.txt\"\n",
    "'''It's taking a long time, don't do it again !'''\n",
    "raise KeyboardInterrupt\n",
    "\n",
    "\n",
    "\n",
    "# We will read them from a file that won't be commited.\n",
    "config = configparser.ConfigParser()\n",
    "_ = config.read('config.ini');\n",
    "consumer_key = config['TWITTER']['consumer_key']\n",
    "consumer_secret = config['TWITTER']['consumer_secret']\n",
    "access_token = config['TWITTER']['access_token']\n",
    "access_secret = config['TWITTER']['access_secret']\n",
    "\n",
    "auth = OAuthHandler(consumer_key, consumer_secret)\n",
    "auth.set_access_token(access_token, access_secret)\n",
    "\n",
    "api = tweepy.API(auth, wait_on_rate_limit=True)\n",
    "\n",
    "tweet_ids = df_twitter_archive_enhanced.tweet_id.values\n",
    "len(tweet_ids)\n",
    "\n",
    "# Query Twitter's API for JSON data for each tweet ID in the Twitter archive\n",
    "count = 0\n",
    "fails_dict = {}\n",
    "start = timer()\n",
    "# Save each tweet's returned JSON as a new line in a .txt file\n",
    "with open(tweets_filename, 'w') as outfile:\n",
    "    # This loop will likely take 20-30 minutes to run because of Twitter's rate limit\n",
    "    for tweet_id in tqdm(tweet_ids):\n",
    "        count += 1\n",
    "        #print(str(count) + \": \" + str(tweet_id))\n",
    "        try:\n",
    "            tweet = api.get_status(tweet_id, tweet_mode='extended')\n",
    "            #print(\"Success\")\n",
    "            _ = json.dump(tweet._json, outfile)\n",
    "            _ = outfile.write('\\n')\n",
    "        except tweepy.TweepError as e:\n",
    "            print(\"Fail\")\n",
    "            fails_dict[tweet_id] = e\n",
    "            pass\n",
    "end = timer()\n",
    "print(end - start)\n",
    "print(fails_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#print(\"1862.885395293939 seconds = \",1862.885395293939/60, \"minutes\")\n",
    "print(\"Time alapse:\",(end - start)/60, \"minutes\")\n",
    "print(\"Number of errors:\", len(fails_dict))\n",
    "#print(\"Errors:\\n\", \"{888202515573088257: TweepError([{'code': 144, 'message': 'No status found with that ID.'}]), 873697596434513921: TweepError([{'code': 144, 'message': 'No status found with that ID.'}]), 872668790621863937: TweepError([{'code': 144, 'message': 'No status found with that ID.'}]), 872261713294495745: TweepError([{'code': 144, 'message': 'No status found with that ID.'}]), 869988702071779329: TweepError([{'code': 144, 'message': 'No status found with that ID.'}]), 866816280283807744: TweepError([{'code': 144, 'message': 'No status found with that ID.'}]), 861769973181624320: TweepError([{'code': 144, 'message': 'No status found with that ID.'}]), 845459076796616705: TweepError([{'code': 144, 'message': 'No status found with that ID.'}]), 842892208864923648: TweepError([{'code': 144, 'message': 'No status found with that ID.'}]), 837012587749474308: TweepError([{'code': 144, 'message': 'No status found with that ID.'}]), 827228250799742977: TweepError([{'code': 144, 'message': 'No status found with that ID.'}]), 812747805718642688: TweepError([{'code': 144, 'message': 'No status found with that ID.'}]), 802247111496568832: TweepError([{'code': 144, 'message': 'No status found with that ID.'}]), 775096608509886464: TweepError([{'code': 144, 'message': 'No status found with that ID.'}]), 770743923962707968: TweepError([{'code': 144, 'message': 'No status found with that ID.'}]), 754011816964026368: TweepError([{'code': 144, 'message': 'No status found with that ID.'}]), 680055455951884288: TweepError([{'code': 144, 'message': 'No status found with that ID.'}])}\")\n",
    "#!ls"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have 17 errors, due to missing tweet, probably deleted.\n",
    "\n",
    "Trying to access it via https://twitter.com/dog_rates/status/888202515573088257 says \"Sorry, that page doesn’t exist!\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load twitter information\n",
    "\n",
    "Let's see what we have in the file :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "'file' n’est pas reconnu en tant que commande interne\n",
      "ou externe, un programme exécutable ou un fichier de commandes.\n",
      "'head' n’est pas reconnu en tant que commande interne\n",
      "ou externe, un programme exécutable ou un fichier de commandes.\n",
      "'tail' n’est pas reconnu en tant que commande interne\n",
      "ou externe, un programme exécutable ou un fichier de commandes.\n"
     ]
    }
   ],
   "source": [
    "!file tweet_json-bco.txt\n",
    "!head -c200 tweet_json-bco.txt\n",
    "!tail -c200 tweet_json-bco.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It's JSON data, but with missing opening { at the begining and missing closing } at the end.\n",
    "So we will read it line by line to process the JSON line-by-line and add them to a Pandas Dataframe.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>favorite_count</th>\n",
       "      <th>retweet_count</th>\n",
       "      <th>tweet_id</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>37653</td>\n",
       "      <td>8208</td>\n",
       "      <td>892420643555336193</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>32351</td>\n",
       "      <td>6072</td>\n",
       "      <td>892177421306343426</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>24372</td>\n",
       "      <td>4014</td>\n",
       "      <td>891815181378084864</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   favorite_count  retweet_count            tweet_id\n",
       "0           37653           8208  892420643555336193\n",
       "1           32351           6072  892177421306343426\n",
       "2           24372           4014  891815181378084864"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "(2339, 3)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "import json\n",
    "# Create an array to receive the data\n",
    "tweets = []\n",
    "with open(tweets_filename, 'r') as infile:\n",
    "    for line in infile:\n",
    "        data = json.loads(line)\n",
    "        # Create a dictionnary from JSON data and add them to the array\n",
    "        tweets.append(dict(tweet_id=data[\"id\"],\n",
    "                               retweet_count=data[\"retweet_count\"],\n",
    "                               favorite_count=data[\"favorite_count\"]))\n",
    "# Load the array in a Dataframe\n",
    "df_tweets = pd.DataFrame(tweets)\n",
    "# Delete the array to save memory\n",
    "del tweets\n",
    "df_tweets.head(3)\n",
    "df_tweets.shape\n",
    "\n",
    "# Pandas has a methode \"read_json\" than could do that for us, with the parameter \"lines=True\".\n",
    "# It works with tweet-json.txt but not with my tweet_json-bco.txt\n",
    "# df_tweets = pd.read_json(tweets_filename, lines=True)\n",
    "# df_tweets.head(3)\n",
    "# df_tweets.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It works. We know have 2 339 lines with 3 columns."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Assessing Data for this Project\n",
    "\n",
    "After gathering each of the above pieces of data, assess them visually and programmatically for quality and tidiness issues. Detect and document at least :\n",
    "- eight (8) quality issues\n",
    "- two (2) tidiness issues"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 2075 entries, 0 to 2074\n",
      "Data columns (total 12 columns):\n",
      "tweet_id    2075 non-null int64\n",
      "jpg_url     2075 non-null object\n",
      "img_num     2075 non-null int64\n",
      "p1          2075 non-null object\n",
      "p1_conf     2075 non-null float64\n",
      "p1_dog      2075 non-null bool\n",
      "p2          2075 non-null object\n",
      "p2_conf     2075 non-null float64\n",
      "p2_dog      2075 non-null bool\n",
      "p3          2075 non-null object\n",
      "p3_conf     2075 non-null float64\n",
      "p3_dog      2075 non-null bool\n",
      "dtypes: bool(3), float64(3), int64(2), object(4)\n",
      "memory usage: 152.1+ KB\n"
     ]
    }
   ],
   "source": [
    "df_pred.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Check : All lines have values in them.\n",
    "And their types where correctly identified."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>tweet_id</th>\n",
       "      <th>img_num</th>\n",
       "      <th>p1_conf</th>\n",
       "      <th>p2_conf</th>\n",
       "      <th>p3_conf</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>2.075000e+03</td>\n",
       "      <td>2075.000000</td>\n",
       "      <td>2075.000000</td>\n",
       "      <td>2.075000e+03</td>\n",
       "      <td>2.075000e+03</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>7.384514e+17</td>\n",
       "      <td>1.203855</td>\n",
       "      <td>0.594548</td>\n",
       "      <td>1.345886e-01</td>\n",
       "      <td>6.032417e-02</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>6.785203e+16</td>\n",
       "      <td>0.561875</td>\n",
       "      <td>0.271174</td>\n",
       "      <td>1.006657e-01</td>\n",
       "      <td>5.090593e-02</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>6.660209e+17</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.044333</td>\n",
       "      <td>1.011300e-08</td>\n",
       "      <td>1.740170e-10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>6.764835e+17</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.364412</td>\n",
       "      <td>5.388625e-02</td>\n",
       "      <td>1.622240e-02</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>7.119988e+17</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.588230</td>\n",
       "      <td>1.181810e-01</td>\n",
       "      <td>4.944380e-02</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>7.932034e+17</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.843855</td>\n",
       "      <td>1.955655e-01</td>\n",
       "      <td>9.180755e-02</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>8.924206e+17</td>\n",
       "      <td>4.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>4.880140e-01</td>\n",
       "      <td>2.734190e-01</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "           tweet_id      img_num      p1_conf       p2_conf       p3_conf\n",
       "count  2.075000e+03  2075.000000  2075.000000  2.075000e+03  2.075000e+03\n",
       "mean   7.384514e+17     1.203855     0.594548  1.345886e-01  6.032417e-02\n",
       "std    6.785203e+16     0.561875     0.271174  1.006657e-01  5.090593e-02\n",
       "min    6.660209e+17     1.000000     0.044333  1.011300e-08  1.740170e-10\n",
       "25%    6.764835e+17     1.000000     0.364412  5.388625e-02  1.622240e-02\n",
       "50%    7.119988e+17     1.000000     0.588230  1.181810e-01  4.944380e-02\n",
       "75%    7.932034e+17     1.000000     0.843855  1.955655e-01  9.180755e-02\n",
       "max    8.924206e+17     4.000000     1.000000  4.880140e-01  2.734190e-01"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_pred.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "min and max of prediction are well in 0-1 range, as expected. And confidence decrease from first to third prediction."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "golden_retriever             150\n",
       "Labrador_retriever           100\n",
       "Pembroke                      89\n",
       "Chihuahua                     83\n",
       "pug                           57\n",
       "chow                          44\n",
       "Samoyed                       43\n",
       "toy_poodle                    39\n",
       "Pomeranian                    38\n",
       "malamute                      30\n",
       "cocker_spaniel                30\n",
       "French_bulldog                26\n",
       "miniature_pinscher            23\n",
       "Chesapeake_Bay_retriever      23\n",
       "seat_belt                     22\n",
       "German_shepherd               20\n",
       "Siberian_husky                20\n",
       "Staffordshire_bullterrier     20\n",
       "Cardigan                      19\n",
       "web_site                      19\n",
       "teddy                         18\n",
       "Eskimo_dog                    18\n",
       "Maltese_dog                   18\n",
       "Shetland_sheepdog             18\n",
       "beagle                        18\n",
       "Lakeland_terrier              17\n",
       "Rottweiler                    17\n",
       "Shih-Tzu                      17\n",
       "Italian_greyhound             16\n",
       "kuvasz                        16\n",
       "                            ... \n",
       "scorpion                       1\n",
       "boathouse                      1\n",
       "African_grey                   1\n",
       "quilt                          1\n",
       "pillow                         1\n",
       "mud_turtle                     1\n",
       "canoe                          1\n",
       "snowmobile                     1\n",
       "sundial                        1\n",
       "cup                            1\n",
       "school_bus                     1\n",
       "bonnet                         1\n",
       "piggy_bank                     1\n",
       "wooden_spoon                   1\n",
       "beach_wagon                    1\n",
       "bow                            1\n",
       "Madagascar_cat                 1\n",
       "sulphur-crested_cockatoo       1\n",
       "microphone                     1\n",
       "cougar                         1\n",
       "crane                          1\n",
       "cowboy_boot                    1\n",
       "coffee_mug                     1\n",
       "maze                           1\n",
       "ibex                           1\n",
       "sliding_door                   1\n",
       "cheetah                        1\n",
       "cheeseburger                   1\n",
       "minibus                        1\n",
       "African_hunting_dog            1\n",
       "Name: p1, Length: 378, dtype: int64"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_pred.p1.value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We saw that some entry have nothing to with dogs : minibus, fountain, desktop_computer, ...\n",
    "But we could identify them with the column \"p1_dog\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True     1532\n",
       "False     543\n",
       "Name: p1_dog, dtype: int64"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_pred.p1_dog.value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So, off 2 750 picture, 543 are identified as not dog."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "golden_retriever               150\n",
       "Labrador_retriever             100\n",
       "Pembroke                        89\n",
       "Chihuahua                       83\n",
       "pug                             57\n",
       "chow                            44\n",
       "Samoyed                         43\n",
       "toy_poodle                      39\n",
       "Pomeranian                      38\n",
       "malamute                        30\n",
       "cocker_spaniel                  30\n",
       "French_bulldog                  26\n",
       "miniature_pinscher              23\n",
       "Chesapeake_Bay_retriever        23\n",
       "Staffordshire_bullterrier       20\n",
       "German_shepherd                 20\n",
       "Siberian_husky                  20\n",
       "Cardigan                        19\n",
       "Maltese_dog                     18\n",
       "Shetland_sheepdog               18\n",
       "Eskimo_dog                      18\n",
       "beagle                          18\n",
       "Lakeland_terrier                17\n",
       "Shih-Tzu                        17\n",
       "Rottweiler                      17\n",
       "Italian_greyhound               16\n",
       "kuvasz                          16\n",
       "West_Highland_white_terrier     14\n",
       "Great_Pyrenees                  14\n",
       "basset                          13\n",
       "                              ... \n",
       "keeshond                         4\n",
       "Afghan_hound                     4\n",
       "Gordon_setter                    4\n",
       "bluetick                         4\n",
       "Mexican_hairless                 4\n",
       "cairn                            3\n",
       "Ibizan_hound                     3\n",
       "Leonberg                         3\n",
       "Welsh_springer_spaniel           3\n",
       "Brabancon_griffon                3\n",
       "Irish_water_spaniel              3\n",
       "curly-coated_retriever           3\n",
       "giant_schnauzer                  3\n",
       "Scottish_deerhound               3\n",
       "briard                           3\n",
       "komondor                         3\n",
       "Greater_Swiss_Mountain_dog       3\n",
       "black-and-tan_coonhound          2\n",
       "Sussex_spaniel                   2\n",
       "wire-haired_fox_terrier          2\n",
       "Appenzeller                      2\n",
       "toy_terrier                      2\n",
       "Australian_terrier               2\n",
       "Scotch_terrier                   1\n",
       "EntleBucher                      1\n",
       "clumber                          1\n",
       "silky_terrier                    1\n",
       "groenendael                      1\n",
       "Japanese_spaniel                 1\n",
       "standard_schnauzer               1\n",
       "Name: p1, Length: 111, dtype: int64"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_pred.query(\"p1_dog == True\")[\"p1\"].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "That looks like dog's bread.\n",
    "Some have capital letter at the beginningn some don't, we could easily change that."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nCheck the datasets sizes\\n'"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "len(df_twitter_archive_enhanced) : 2356\n",
      "len(df_tweets) : 2339\n",
      "len(df_pred) : 2075\n",
      "len(df_twitter_archive_enhanced) - len(df_tweets) : 17\n"
     ]
    }
   ],
   "source": [
    "'''\n",
    "Check the datasets sizes\n",
    "'''\n",
    "pv(\"len(df_twitter_archive_enhanced)\")\n",
    "pv(\"len(df_tweets)\")\n",
    "pv(\"len(df_pred)\")\n",
    "pv(\"len(df_twitter_archive_enhanced) - len(df_tweets)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We do not have the same size for all dataset.\n",
    "\n",
    "Differences between df_twitter_archive_enhanced and df_tweets are the 17 deleted tweets.\n",
    "\n",
    "Image prediction is the smallest dataset. But still above 2 000, so we could do a strict merge and keep only the matching tweet ID of every dataset.\n",
    "\n",
    "But, image prediction are done for every image in the tweet, as we could have up to 4 images for a tweet we will not merge it, instead we will keep it appart, and use the tweet_id as a key."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nWe are going to merge the df_twitter_archive_enhanced and df_tweets datasets in one.\\n'"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Archive enhanced :\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Index(['tweet_id', 'in_reply_to_status_id', 'in_reply_to_user_id', 'timestamp',\n",
       "       'source', 'text', 'retweeted_status_id', 'retweeted_status_user_id',\n",
       "       'retweeted_status_timestamp', 'expanded_urls', 'rating_numerator',\n",
       "       'rating_denominator', 'name', 'doggo', 'floofer', 'pupper', 'puppo'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "From tweeter API : \n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Index(['favorite_count', 'retweet_count', 'tweet_id'], dtype='object')"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image prediction : \n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Index(['tweet_id', 'jpg_url', 'img_num', 'p1', 'p1_conf', 'p1_dog', 'p2',\n",
       "       'p2_conf', 'p2_dog', 'p3', 'p3_conf', 'p3_dog'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "(2075, 12)"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "(2339, 19)"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "We are going to merge the df_twitter_archive_enhanced and df_tweets datasets in one.\n",
    "'''\n",
    "print(\"Archive enhanced :\")\n",
    "#df_twitter_archive_enhanced.head(3)\n",
    "df_twitter_archive_enhanced.columns\n",
    "print(\"From tweeter API : \")\n",
    "#df_tweets.head(3)\n",
    "df_tweets.columns\n",
    "print(\"Image prediction : \")\n",
    "#df_pred.head(3)\n",
    "df_pred.columns\n",
    "df_pred.shape\n",
    "\n",
    "\n",
    "df_twitter_all = pd.merge(df_twitter_archive_enhanced, df_tweets, on='tweet_id')\n",
    "#df_twitter_all.head(3)\n",
    "df_twitter_all.shape\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Int64Index: 2339 entries, 0 to 2338\n",
      "Data columns (total 19 columns):\n",
      "tweet_id                      2339 non-null int64\n",
      "in_reply_to_status_id         78 non-null float64\n",
      "in_reply_to_user_id           78 non-null float64\n",
      "timestamp                     2339 non-null object\n",
      "source                        2339 non-null object\n",
      "text                          2339 non-null object\n",
      "retweeted_status_id           167 non-null float64\n",
      "retweeted_status_user_id      167 non-null float64\n",
      "retweeted_status_timestamp    167 non-null object\n",
      "expanded_urls                 2280 non-null object\n",
      "rating_numerator              2339 non-null int64\n",
      "rating_denominator            2339 non-null int64\n",
      "name                          2339 non-null object\n",
      "doggo                         2339 non-null object\n",
      "floofer                       2339 non-null object\n",
      "pupper                        2339 non-null object\n",
      "puppo                         2339 non-null object\n",
      "favorite_count                2339 non-null int64\n",
      "retweet_count                 2339 non-null int64\n",
      "dtypes: float64(4), int64(5), object(10)\n",
      "memory usage: 365.5+ KB\n"
     ]
    }
   ],
   "source": [
    "df_twitter_all.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- timestamp and retweeted_status_timestamp are \"object\" (i.e. string) so we will have to convert them to datetime.\n",
    "- in_reply_to_status_id, in_reply_to_user_id, retweeted_status_id, retweeted_status_user_id  are Float64, why not Int64 ?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Automatic select column to check\n",
    "Check the number of disctinct values in every columns, then add the column to a list if they are less than 50 distinct values.\n",
    "\n",
    "So we could display them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Distinct values in tweet_id : 2339\n",
      "Distinct values in in_reply_to_status_id : 77\n",
      "Distinct values in in_reply_to_user_id : 31\n",
      "Distinct values in timestamp : 2339\n",
      "Distinct values in source : 4\n",
      "Distinct values in text : 2339\n",
      "Distinct values in retweeted_status_id : 167\n",
      "Distinct values in retweeted_status_user_id : 23\n",
      "Distinct values in retweeted_status_timestamp : 167\n",
      "Distinct values in expanded_urls : 2208\n",
      "Distinct values in rating_numerator : 40\n",
      "Distinct values in rating_denominator : 18\n",
      "Distinct values in name : 956\n",
      "Distinct values in doggo : 2\n",
      "Distinct values in floofer : 2\n",
      "Distinct values in pupper : 2\n",
      "Distinct values in puppo : 2\n",
      "Distinct values in favorite_count : 1986\n",
      "Distinct values in retweet_count : 1715\n",
      "\n",
      "Column with too many distinct values :\n",
      " ['tweet_id', 'in_reply_to_status_id', 'timestamp', 'text', 'retweeted_status_id', 'retweeted_status_timestamp', 'expanded_urls', 'name', 'favorite_count', 'retweet_count']\n"
     ]
    }
   ],
   "source": [
    "columns = df_twitter_all.columns\n",
    "to_be_removed = []\n",
    "for column in columns:\n",
    "    nb_unique_values = eval('df_twitter_all.' + column + '.nunique()')\n",
    "    print(\"Distinct values in\", column, \":\", nb_unique_values)\n",
    "    if nb_unique_values > 50:\n",
    "        to_be_removed.append(column)\n",
    "print(\"\\nColumn with too many distinct values :\\n\", to_be_removed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nDisplay distinct values\\n'"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Distinct values in in_reply_to_user_id\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "4.196984e+09    47\n",
       "2.195506e+07     2\n",
       "7.305050e+17     1\n",
       "2.916630e+07     1\n",
       "3.105441e+09     1\n",
       "2.918590e+08     1\n",
       "2.792810e+08     1\n",
       "2.319108e+09     1\n",
       "1.806710e+08     1\n",
       "3.058208e+07     1\n",
       "2.625958e+07     1\n",
       "1.943518e+08     1\n",
       "3.589728e+08     1\n",
       "8.405479e+17     1\n",
       "2.894131e+09     1\n",
       "2.143566e+07     1\n",
       "2.281182e+09     1\n",
       "1.648776e+07     1\n",
       "4.717297e+09     1\n",
       "2.878549e+07     1\n",
       "1.582854e+09     1\n",
       "4.670367e+08     1\n",
       "4.738443e+07     1\n",
       "1.361572e+07     1\n",
       "1.584641e+07     1\n",
       "2.068372e+07     1\n",
       "1.637468e+07     1\n",
       "1.185634e+07     1\n",
       "1.198989e+09     1\n",
       "1.132119e+08     1\n",
       "7.759620e+07     1\n",
       "Name: in_reply_to_user_id, dtype: int64"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Distinct values in source\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<a href=\"http://twitter.com/download/iphone\" rel=\"nofollow\">Twitter for iPhone</a>     2205\n",
       "<a href=\"http://vine.co\" rel=\"nofollow\">Vine - Make a Scene</a>                          91\n",
       "<a href=\"http://twitter.com\" rel=\"nofollow\">Twitter Web Client</a>                       33\n",
       "<a href=\"https://about.twitter.com/products/tweetdeck\" rel=\"nofollow\">TweetDeck</a>      10\n",
       "Name: source, dtype: int64"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Distinct values in retweeted_status_user_id\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "4.196984e+09    144\n",
       "4.296832e+09      2\n",
       "5.870972e+07      1\n",
       "6.669901e+07      1\n",
       "4.119842e+07      1\n",
       "7.475543e+17      1\n",
       "7.832140e+05      1\n",
       "4.871977e+08      1\n",
       "5.970642e+08      1\n",
       "4.466750e+07      1\n",
       "1.228326e+09      1\n",
       "7.992370e+07      1\n",
       "2.488557e+07      1\n",
       "7.874618e+17      1\n",
       "3.638908e+08      1\n",
       "5.128045e+08      1\n",
       "8.117408e+08      1\n",
       "1.732729e+09      1\n",
       "1.960740e+07      1\n",
       "3.410211e+08      1\n",
       "7.124572e+17      1\n",
       "2.804798e+08      1\n",
       "1.950368e+08      1\n",
       "Name: retweeted_status_user_id, dtype: int64"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Distinct values in rating_numerator\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "12      554\n",
       "11      464\n",
       "10      459\n",
       "13      344\n",
       "9       157\n",
       "8       102\n",
       "7        55\n",
       "14       51\n",
       "5        37\n",
       "6        32\n",
       "3        19\n",
       "4        17\n",
       "1         9\n",
       "2         9\n",
       "420       2\n",
       "0         2\n",
       "15        2\n",
       "75        2\n",
       "80        1\n",
       "20        1\n",
       "24        1\n",
       "26        1\n",
       "44        1\n",
       "50        1\n",
       "60        1\n",
       "165       1\n",
       "84        1\n",
       "88        1\n",
       "144       1\n",
       "182       1\n",
       "143       1\n",
       "666       1\n",
       "960       1\n",
       "1776      1\n",
       "17        1\n",
       "27        1\n",
       "45        1\n",
       "99        1\n",
       "121       1\n",
       "204       1\n",
       "Name: rating_numerator, dtype: int64"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Distinct values in rating_denominator\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "10     2317\n",
       "50        3\n",
       "80        2\n",
       "11        2\n",
       "20        2\n",
       "2         1\n",
       "16        1\n",
       "40        1\n",
       "70        1\n",
       "15        1\n",
       "90        1\n",
       "110       1\n",
       "120       1\n",
       "130       1\n",
       "150       1\n",
       "170       1\n",
       "7         1\n",
       "0         1\n",
       "Name: rating_denominator, dtype: int64"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Distinct values in doggo\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "None     2243\n",
       "doggo      96\n",
       "Name: doggo, dtype: int64"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Distinct values in floofer\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "None       2329\n",
       "floofer      10\n",
       "Name: floofer, dtype: int64"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Distinct values in pupper\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "None      2083\n",
       "pupper     256\n",
       "Name: pupper, dtype: int64"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Distinct values in puppo\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "None     2309\n",
       "puppo      30\n",
       "Name: puppo, dtype: int64"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "Display distinct values\n",
    "'''\n",
    "columns_redux = [item for item in columns if item not in to_be_removed ]\n",
    "for column in columns_redux:\n",
    "    print(\"Distinct values in\", column)\n",
    "    eval('df_twitter_all.' + column + '.value_counts()')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With the above we already see some issues :\n",
    "- \"source\" include HTML tag, but even without them, it do not seems of any use, I think there is not the values we expected in it.\n",
    "- rating_numerator and rating_denominator need a closer look.\n",
    "- Dog stages are in separate column\n",
    "- Dog stages have few values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nShow 15 most given names\\n'"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "None 738\n",
      "a 55\n",
      "Charlie 11\n",
      "Cooper 11\n",
      "Lucy 11\n",
      "Oliver 11\n",
      "Lola 10\n",
      "Tucker 10\n",
      "Penny 10\n",
      "Winston 9\n",
      "Bo 9\n",
      "Sadie 8\n",
      "the 8\n",
      "an 7\n",
      "Buddy 7\n"
     ]
    }
   ],
   "source": [
    "'''\n",
    "Show 15 most given names\n",
    "'''\n",
    "import itertools\n",
    "# Thanks to https://stackoverflow.com/questions/36106712/how-can-i-limit-iterations-of-a-loop-in-python\n",
    "for name, cnt in itertools.islice(df_twitter_all.name.value_counts().iteritems(),0,15):\n",
    "      print(name, cnt)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\"None\" is missing name, they are many of them, looking at the file in a editor, they seems to be extracted only with the pattern \"This is &lt;dog_name&gt;\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nShow names with less than 3 caracters to check them.\\n'"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "a 55\n",
      "Bo 9\n",
      "an 7\n",
      "JD 1\n",
      "O 1\n",
      "by 1\n",
      "Ed 1\n",
      "Al 1\n",
      "Jo 1\n",
      "my 1\n",
      "Mo 1\n"
     ]
    }
   ],
   "source": [
    "'''\n",
    "Show names with less than 3 caracters to check them.\n",
    "'''\n",
    "names = df_twitter_all.name.value_counts()\n",
    "# Thanks to https://stackoverflow.com/questions/36973387/accessing-first-column-of-pandas-value-counts\n",
    "for name, cnt in names.iteritems():\n",
    "    if len(name)<3:\n",
    "        print(name, cnt)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see that \"a\", \"an\", \"by\", \"my\", \"the\", \"O\" are not the names of the dogs. But explained by the fact only ther first word after \"This is\" is extracted."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nHave a look to notation outliers\\n'"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "10      459\n",
       "9       157\n",
       "8       102\n",
       "7        55\n",
       "5        37\n",
       "6        32\n",
       "3        19\n",
       "4        17\n",
       "1         9\n",
       "2         9\n",
       "75        2\n",
       "0         2\n",
       "420       2\n",
       "27        1\n",
       "17        1\n",
       "20        1\n",
       "24        1\n",
       "26        1\n",
       "960       1\n",
       "44        1\n",
       "1776      1\n",
       "50        1\n",
       "60        1\n",
       "80        1\n",
       "84        1\n",
       "88        1\n",
       "99        1\n",
       "121       1\n",
       "143       1\n",
       "144       1\n",
       "165       1\n",
       "182       1\n",
       "204       1\n",
       "666       1\n",
       "45        1\n",
       "Name: rating_numerator, dtype: int64"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "50     3\n",
       "20     2\n",
       "80     2\n",
       "11     2\n",
       "90     1\n",
       "40     1\n",
       "130    1\n",
       "2      1\n",
       "70     1\n",
       "7      1\n",
       "110    1\n",
       "170    1\n",
       "120    1\n",
       "15     1\n",
       "16     1\n",
       "150    1\n",
       "0      1\n",
       "Name: rating_denominator, dtype: int64"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "Have a look to notation outliers\n",
    "'''\n",
    "df_twitter_rating_numerator = df_twitter_all.query(\"rating_numerator < 11 or rating_numerator > 15\")\n",
    "df_twitter_rating_denominator = df_twitter_all.query(\"rating_denominator != 10\")\n",
    "df_twitter_rating_numerator.rating_numerator.value_counts()\n",
    "df_twitter_rating_denominator.rating_denominator.value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The notations are not always between 11 and 14 like we expect, let's have a closer look."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nHave a closer look to notation outliers\\n'"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------------------------------------------\n",
      "tweet : @dhmontgomery We also gave snoop dogg a 420/10 but I think that predated your research\n",
      "------------------------------------------\n",
      "tweet : @s8n You tried very hard to portray this good boy as not so good, but you have ultimately failed. His goodness shines through. 666/10\n",
      "------------------------------------------\n",
      "tweet : @markhoppus 182/10\n",
      "------------------------------------------\n",
      "tweet : @jonnysun @Lin_Manuel ok jomny I know you're excited but 960/00 isn't a valid rating, 13/10 is tho\n",
      "------------------------------------------\n",
      "tweet : RT @dog_rates: This is Logan, the Chow who lived. He solemnly swears he's up to lots of good. H*ckin magical af 9.75/10 https://t.co/yBO5wu…\n",
      "------------------------------------------\n",
      "tweet : The floofs have been released I repeat the floofs have been released. 84/70 https://t.co/NIYC820tmd\n",
      "------------------------------------------\n",
      "tweet : This is Logan, the Chow who lived. He solemnly swears he's up to lots of good. H*ckin magical af 9.75/10 https://t.co/yBO5wuqaPS\n",
      "------------------------------------------\n",
      "tweet : Why does this never happen at my front door... 165/150 https://t.co/HmwrdfEfUE\n",
      "------------------------------------------\n",
      "tweet : This is Atticus. He's quite simply America af. 1776/10 https://t.co/GRXwMxLBkh\n",
      "------------------------------------------\n",
      "tweet : Say hello to this unbelievably well behaved squad of doggos. 204/170 would try to pet all at once https://t.co/yGQI3He3xv\n",
      "------------------------------------------\n",
      "tweet : Happy Saturday here's 9 puppers on a bench. 99/90 good work everybody https://t.co/mpvaVxKmc1\n",
      "------------------------------------------\n",
      "tweet : Here's a brigade of puppers. All look very prepared for whatever happens next. 80/80 https://t.co/0eb7R1Om12\n",
      "------------------------------------------\n",
      "tweet : Here is a whole flock of puppers.  60/50 I'll take the lot https://t.co/9dpcw6MdWa\n",
      "------------------------------------------\n",
      "tweet : Two sneaky puppers were not initially seen, moving the rating to 143/130. Please forgive us. Thank you https://t.co/kRK51Y5ac3\n",
      "------------------------------------------\n",
      "tweet : Someone help the girl is being mugged. Several are distracting her while two steal her shoes. Clever puppers 121/110 https://t.co/1zfnTJLt55\n",
      "------------------------------------------\n",
      "tweet : IT'S PUPPERGEDDON. Total of 144/120 ...I think https://t.co/ZanVtAtvIq\n",
      "------------------------------------------\n",
      "tweet : Here we have an entire platoon of puppers. Total score: 88/80 would pet all at once https://t.co/y93p6FLvVw\n",
      "------------------------------------------\n",
      "tweet : After so many requests... here you go.\n",
      "\n",
      "Good dogg. 420/10 https://t.co/yfAAo1gdeY\n",
      "------------------------------------------\n",
      "tweet : @jonnysun @Lin_Manuel ok jomny I know you're excited but 960/00 isn't a valid rating, 13/10 is tho\n",
      "------------------------------------------\n",
      "tweet : @docmisterio account started on 11/15/15\n",
      "------------------------------------------\n",
      "tweet : The floofs have been released I repeat the floofs have been released. 84/70 https://t.co/NIYC820tmd\n",
      "------------------------------------------\n",
      "tweet : Meet Sam. She smiles 24/7 &amp; secretly aspires to be a reindeer. \n",
      "Keep Sam smiling by clicking and sharing this link:\n",
      "https://t.co/98tB8y7y7t https://t.co/LouL5vdvxx\n",
      "------------------------------------------\n",
      "tweet : Why does this never happen at my front door... 165/150 https://t.co/HmwrdfEfUE\n",
      "------------------------------------------\n",
      "tweet : After so many requests, this is Bretagne. She was the last surviving 9/11 search dog, and our second ever 14/10. RIP https://t.co/XAVDNDaVgQ\n",
      "------------------------------------------\n",
      "tweet : Say hello to this unbelievably well behaved squad of doggos. 204/170 would try to pet all at once https://t.co/yGQI3He3xv\n",
      "------------------------------------------\n",
      "tweet : Happy 4/20 from the squad! 13/10 for all https://t.co/eV1diwds8a\n",
      "------------------------------------------\n",
      "tweet : This is Bluebert. He just saw that both #FinalFur match ups are split 50/50. Amazed af. 11/10 https://t.co/Kky1DPG4iq\n",
      "------------------------------------------\n",
      "tweet : Happy Saturday here's 9 puppers on a bench. 99/90 good work everybody https://t.co/mpvaVxKmc1\n",
      "------------------------------------------\n",
      "tweet : Here's a brigade of puppers. All look very prepared for whatever happens next. 80/80 https://t.co/0eb7R1Om12\n",
      "------------------------------------------\n",
      "tweet : From left to right:\n",
      "Cletus, Jerome, Alejandro, Burp, &amp; Titson\n",
      "None know where camera is. 45/50 would hug all at once https://t.co/sedre1ivTK\n",
      "------------------------------------------\n",
      "tweet : Here is a whole flock of puppers.  60/50 I'll take the lot https://t.co/9dpcw6MdWa\n",
      "------------------------------------------\n",
      "tweet : Happy Wednesday here's a bucket of pups. 44/40 would pet all at once https://t.co/HppvrYuamZ\n",
      "------------------------------------------\n",
      "tweet : Yes I do realize a rating of 4/20 would've been fitting. However, it would be unjust to give these cooperative pups that low of a rating\n",
      "------------------------------------------\n",
      "tweet : Two sneaky puppers were not initially seen, moving the rating to 143/130. Please forgive us. Thank you https://t.co/kRK51Y5ac3\n",
      "------------------------------------------\n",
      "tweet : Someone help the girl is being mugged. Several are distracting her while two steal her shoes. Clever puppers 121/110 https://t.co/1zfnTJLt55\n",
      "------------------------------------------\n",
      "tweet : This is Darrel. He just robbed a 7/11 and is in a high speed police chase. Was just spotted by the helicopter 10/10 https://t.co/7EsP8LmSp5\n",
      "------------------------------------------\n",
      "tweet : I'm aware that I could've said 20/16, but here at WeRateDogs we are very professional. An inconsistent rating scale is simply irresponsible\n",
      "------------------------------------------\n",
      "tweet : IT'S PUPPERGEDDON. Total of 144/120 ...I think https://t.co/ZanVtAtvIq\n",
      "------------------------------------------\n",
      "tweet : Here we have an entire platoon of puppers. Total score: 88/80 would pet all at once https://t.co/y93p6FLvVw\n",
      "------------------------------------------\n",
      "tweet : This is an Albanian 3 1/2 legged  Episcopalian. Loves well-polished hardwood flooring. Penis on the collar. 9/10 https://t.co/d9NcXFKwLv\n"
     ]
    }
   ],
   "source": [
    "'''\n",
    "Have a closer look to notation outliers\n",
    "'''\n",
    "df_poor_notation = df_twitter_all.loc[df_twitter_all['rating_numerator'] > 50]\n",
    "#df_poor_notation = df_poor_notation[['tweet_id', 'rating_numerator', 'text']]\n",
    "\n",
    "for tweet in df_poor_notation['text']:\n",
    "    print(\"------------------------------------------\\ntweet :\", tweet)\n",
    "\n",
    "df_poor_notation = df_twitter_all.loc[df_twitter_all['rating_denominator'] != 10]\n",
    "#df_poor_notation = df_poor_notation[['tweet_id', 'rating_numerator', 'text']]\n",
    "\n",
    "for tweet in df_poor_notation['text']:\n",
    "    print(\"------------------------------------------\\ntweet :\", tweet)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We saw that the outliers are real notation, given the fact that the whole thing is a joke.\n",
    "There is few extraction errors :\n",
    "- At least we could correct the notation extration to properly get decimal notation like \"9.75\".\n",
    "- Sometimes there is more than one notation in a sentence. We could keep the last as the right notation.\n",
    "- Sometimes a fraction is used and took as a notation but is not : for exemple \"1/2 legged\"\n",
    "\n",
    "- We saw than some tweet have there short url in it, at the end, that is useless."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "### Quality issues\n",
    "\n",
    "To sum up, I found all these quality issues with content :\n",
    "\n",
    "1. rating_denominator containt some values different from \"10\". But looking at the concerned tweets some are \"real\" notation.\n",
    "2. rating_numerator have values from 0 to 420.\n",
    "3. Some tweet have theyre short url included, at the end, that is useless.\n",
    "4. Dog names are often None, because they seems to be extracted only with the pattern \"This is <dog_name>\"\n",
    "5. Dog stages are often None. \n",
    "6. Retweets has to be removed\n",
    "7. \"source\" include HTML tag, but even without them, it do not seems of any use, I think there is not the values we expected in it.\n",
    "8. timestamp and retweeted_status_timestamp are \"object\" (i.e. string) so we will have to convert them to datetime. \n",
    "9. 17 tweet have been deleted.\n",
    "10. in_reply_to_status_id, in_reply_to_user_id, retweeted_status_id and retweeted_status_user_id have been incorectly stored as Float64,  with values like 8.862663570751283e+17 or 2281181600.0\n",
    "11. Dogs names are wrongly extracted with name like \"a\", \"an\", \"by\", \"my\", \"the\" and \"O\".\n",
    "12. rating_numerator incorectly extract decimal notation \"9.75\" became \"75\".\n",
    "13. Sometimes there is more than one notation in a sentence. Looking at tweets the provided extraction took the first notation but it seeems better to keep the last.\n",
    "14. Sometimes a fraction is used and took as a notation but is not : for exemple \"1/2 legged\"\n",
    "15. Sometimes more than one dog is concerned : \"Here we have Burke (pupper) and Dexter (doggo)\"\n",
    "16. None is stored as a string and not as None Type, nor NaN.\n",
    "\n",
    "\n",
    "\n",
    "### Tidiness issues\n",
    "These are issues with structure that prevent easy analysis. Untidy data is also known as **messy data**. Tidy data requires:\n",
    "- Each variable forms a column.\n",
    "- Each observation forms a row.\n",
    "- Each type of observational unit forms a table.\n",
    "\n",
    "Most commont problems :\n",
    "- Column headers are values, not variable names.\n",
    "- Multiple variables are stored in one column.\n",
    "- Variables are stored in both rows and columns.\n",
    "- Multiple types of observational units are stored in the same table.\n",
    "- A single observational unit is stored in multiple tables.\n",
    "\n",
    "\n",
    "I've found the following tidiness issues :\n",
    "1. expanded_urls may contain up to 4 comma separated url to the images. We better have to put them in another table. The same as prediction on images.\n",
    "2. The dog stages is not in a usefull format, having as many column as dog type like \"doggo, floofer, pupper, puppo\". So we better have to put it in one column \"dog_stages\". If we want to train a model we still could create dummies variables from the new columns.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Cleaning\n",
    "\n",
    "We first make a copy of the data set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_twitter_clean = df_twitter_all.copy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Remove URLs in tweets\n",
    "We saw than some tweet have there short url in it.\n",
    "We are going to remove all url in tweets.\n",
    "Warning : it may not be what we want if we want to compute some real link posted in a tweet. If so we could change the regexp to get only the url at the end.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nWe do a loop over the cleaned text to check they are no more URL\\n'"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "All URLs removed !\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "# RegExp to find URLs from http://www.noah.org/wiki/RegEx_Python#URL_regex_pattern\n",
    "pattern = re.compile(r'http[s]?://(?:[a-zA-Z]|[0-9]|[$-_@.&+]|[!*\\(\\),]|(?:%[0-9a-fA-F][0-9a-fA-F]))+')\n",
    "\n",
    "# Define a methode to remove all the URLs in a string.\n",
    "def remove_url(text):\n",
    "    notations = re.finditer(pattern, text) # search for first, finditer for all\n",
    "    # We have a for loop because sometimes there are more than one url.\n",
    "    for notation in notations:\n",
    "        text = text.replace(notation.group(), '')\n",
    "    return text\n",
    "\n",
    "# Apply the method to all \n",
    "df_twitter_clean = df_twitter_clean.drop('text', axis=1)\n",
    "df_twitter_clean['text'] = df_twitter_all['text'].apply(remove_url)\n",
    "\n",
    "\n",
    "'''\n",
    "We do a loop over the cleaned text to check they are no more URL\n",
    "'''\n",
    "# Check it has worked on all url\n",
    "fail = False\n",
    "for tweet in df_twitter_clean['text']:\n",
    "    notations = re.search(pattern, tweet)\n",
    "    if notations:\n",
    "        print(\"ERROR, still URL\")\n",
    "        fail = True\n",
    "        print(notations.group())\n",
    "        new_tweet = tweet.replace(notations.group(), '')\n",
    "        print(new_tweet)\n",
    "        pos = tweet.find(one_notation.group())\n",
    "        start = pos\n",
    "        end = pos+50\n",
    "        notation = tweet[start:end]\n",
    "        print(\"notation:\", notation)\n",
    "if not fail:\n",
    "    print(\"All URLs removed !\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cleaning notation\n",
    "\n",
    "\n",
    "rating_denominator containt some values different from \"10\". But looking at the concerned tweets these are \"real\" notation.\n",
    "\n",
    "rating_numerator have values from 0 to 420. Only few of them are extraction mistake, like this one where a 9.5/10 became a 5/10 :\n",
    "\"I've been told there's a slight possibility he's checking his mirror. We'll bump to 9.5/10. Still a menace\"\n",
    "Or missreading when more than one notation :\n",
    "\"@jonnysun @Lin_Manuel ok jomny I know you're excited but 960/00 isn't a valid rating, 13/10 is tho\"\n",
    "\n",
    "@dog_rates sometime goes out of it classic notation between 11 to 15 :\n",
    "\"His goodness shines through. 666/10\"\n",
    "\"He's quite simply America af. 1776/10\"\n",
    "\n",
    "So I will programaticaly corrected the few real extraction errors. We will use a better regex to extract the notation.\n",
    "\n",
    "But if we want to do calculus one the notation, it seems fair to remove the outlier made of jokes to keep only the common notation from 11 to 15. It could be simply done with :\n",
    "df_twitter_tmp = df_twitter_all.query(\"rating_numerator > 10 and rating_numerator <= 15 and rating_denominator == 10\")\n",
    "But as these are jokes anyway, we have to think if it make any sense to do calculus.\n",
    "\n",
    "Reminder :\n",
    "**The fact that the rating numerators are greater than the denominators does not need to be cleaned.** This unique rating system is a big part of the popularity of WeRateDogs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Rename tidy columns\n",
    "#df_twitter_clean.columns\n",
    "\n",
    "#df_twitter_clean.columns\n",
    "# Get the new notation\n",
    "# Thanks to http://jonathansoma.com/lede/foundations/classes/pandas%20columns%20and%20functions/apply-a-function-to-every-row-in-a-pandas-dataframe/\n",
    "#  for how to apply a method to a whole row and save back the result\n",
    "notation_pattern = re.compile(r'(?=\\d)\\d*\\.?\\d+/\\d*\\.?\\d+') # RegExp to extract notation that handle decimal.\n",
    "def get_rating_numerator(row):\n",
    "    rating = None\n",
    "    notations = re.finditer(notation_pattern, row['text'])\n",
    "    #print(row['text'])\n",
    "    if notations:\n",
    "        for one_notation in notations:\n",
    "            notation = one_notation.group()\n",
    "            #print(\"Notation found:\", notation)\n",
    "            rating = float(notation.split('/')[0])\n",
    "            #print(\"get_rating_numerator\", rating)\n",
    "    else:\n",
    "        print(\"No notation !!!\")\n",
    "        rating = None\n",
    "    return rating\n",
    "\n",
    "def get_rating_denominator(row):\n",
    "    notations = re.finditer(notation_pattern, row['text'])\n",
    "    rating = None\n",
    "    if notations:\n",
    "        for one_notation in notations:\n",
    "            notation = one_notation.group()\n",
    "            #print(\"Notation found:\", notation)\n",
    "            rating = float(notation.split('/')[1])\n",
    "            #print(\"get_rating_denominator\", rating)\n",
    "    else:\n",
    "        print(\"No notation !!!\")\n",
    "    return rating\n",
    "\n",
    "#df_poor_notation = df_twitter_all.loc[df_twitter_all['rating_numerator'] > 50]\n",
    "\n",
    "df_twitter_clean['new_rating_numerator'] = df_twitter_all.apply(get_rating_numerator, axis=1)\n",
    "df_twitter_clean['new_rating_denominator'] = df_twitter_all.apply(get_rating_denominator, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---\n",
      " 5 / 10 is now 13.5 / 10.0  Text : This is Bella. She hopes her smile made you smile. If not, she is also offering you her favorite monkey. 13.5/10 \n",
      "---\n",
      " 17 / 10 is now 13.0 / 10.0  Text : @roushfenway These are good dogs but 17/10 is an emotional impulse rating. More like 13/10s\n",
      "---\n",
      " 960 / 0 is now 13.0 / 10.0  Text : @jonnysun @Lin_Manuel ok jomny I know you're excited but 960/00 isn't a valid rating, 13/10 is tho\n",
      "---\n",
      " 75 / 10 is now 9.75 / 10.0  Text : RT @dog_rates: This is Logan, the Chow who lived. He solemnly swears he's up to lots of good. H*ckin magical af 9.75/10 …\n",
      "---\n",
      " 12 / 10 is now 11.0 / 10.0  Text : RT @dog_rates: \"Yep... just as I suspected. You're not flossing.\" 12/10 and 11/10 for the pup not flossing \n"
     ]
    }
   ],
   "source": [
    "df_temp = df_twitter_clean.query(\"rating_numerator != new_rating_numerator or rating_denominator != new_rating_denominator\")\n",
    "\n",
    "import itertools\n",
    "# Thanks to https://stackoverflow.com/questions/36106712/how-can-i-limit-iterations-of-a-loop-in-python\n",
    "for index, row in itertools.islice(df_temp[['new_rating_numerator', 'new_rating_denominator', 'text','rating_numerator', 'rating_denominator']].iterrows(),0,5):\n",
    "    print(\"---\\n\", row['rating_numerator'], '/', row['rating_denominator'], \"is now\", row['new_rating_numerator'], '/', row['new_rating_denominator'], \" Text :\",row['text'])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Well, it looks good, but I'm not sure taking the last notation is better than the first.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Clean Dog Name"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Show the none\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['tweet_id', 'in_reply_to_status_id', 'in_reply_to_user_id', 'timestamp',\n",
       "       'source', 'text', 'retweeted_status_id', 'retweeted_status_user_id',\n",
       "       'retweeted_status_timestamp', 'expanded_urls', 'rating_numerator',\n",
       "       'rating_denominator', 'name', 'doggo', 'floofer', 'pupper', 'puppo',\n",
       "       'favorite_count', 'retweet_count'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_twitter_all.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---\n",
      " None / Here we have a majestic great white breaching off South Africa's coast. Absolutely h*ckin breathtaking. 13/10 (IG: tucker_marlo) #BarkWeek https://t.co/kQ04fDDRmh\n",
      "---\n",
      " None / When you watch your owner call another dog a good boy but then they turn back to you and say you're a great boy. 13/10 https://t.co/v0nONBcwxq\n",
      "---\n",
      " None / Here's a puppo that seems to be on the fence about something haha no but seriously someone help her. 13/10 https://t.co/BxvuXk0UCm\n"
     ]
    }
   ],
   "source": [
    "import itertools\n",
    "# Thanks to https://stackoverflow.com/questions/36106712/how-can-i-limit-iterations-of-a-loop-in-python\n",
    "for index, row in itertools.islice(df_twitter_all.query(\"name == 'None'\")[['name', 'text']].iterrows(),0,3):\n",
    "    print(\"---\\n\", row['name'], '/', row['text'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we have Pancho\n",
    "This is Spark\n",
    "Meet Hank\n",
    "This a Norwegian Pewterschmidt named Tickles\n",
    "Here we have an Azerbaijani Buttermilk named Guss\n",
    "This lil pup is Oliver\n",
    "Their names are Cupit and Prencer\n",
    "Say hello to \n",
    "\n",
    "There are not often a name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Regex to get the name, tryed on https://pythex.org/\n",
    "name_pattern = re.compile(r\"(?:named| is|[mM]eet|have| are| hello to) [A-Z]'*[a-zA-Záàâäãåçéèêëíìîïñóòôöõúùûüýÿæœ]+\")\n",
    "# Method to get the name from a row\n",
    "def get_dog_name(row):\n",
    "    name = 'None'\n",
    "    names = re.finditer(name_pattern, row['text'])\n",
    "    one_name = 'None'\n",
    "    for one_name in names:\n",
    "        name = one_name.group()\n",
    "        #print(name)\n",
    "        name = name.split(' ')[-1]\n",
    "    if one_name is None:\n",
    "        #print(\"No name !!!\")\n",
    "        name = 'None'\n",
    "    return name\n",
    "\n",
    "df_twitter_clean['new_name'] = df_twitter_all.apply(get_dog_name, axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The regex is quite complicated because we handle the special name O'Maley and Oliviér for exemple.\n",
    "\n",
    "But not \"Big Jumpy Rat\" for exemple. Nor \"This is Amy. She is Queen\" where we get Queen instead of Amy.\n",
    "All in all, it is much better."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " such => None  Tweet: I've yet to rate a Venezuelan Hover Wiener. This is such an honor. 14/10 paw-inspiring af (IG: roxy.thedoxy) \n",
      "\n",
      " None => Howard  Tweet: I have a new hero and his name is Howard. 14/10 \n",
      "\n",
      " a => None  Tweet: Here is a pupper approaching maximum borkdrive. Zooming at never before seen speeds. 14/10 paw-inspiring af \n",
      "(IG: puffie_the_chow) \n",
      "\n",
      " None => Zoey  Tweet: Sorry for the lack of posts today. I came home from school and had to spend quality time with my puppo. Her name is Zoey and she's 13/10 \n",
      "\n",
      " quite => None  Tweet: We only rate dogs. This is quite clearly a smol broken polar bear. We'd appreciate if you only send dogs. Thank you... 12/10 \n",
      "\n",
      " quite => None  Tweet: Guys, we only rate dogs. This is quite clearly a bulbasaur. Please only send dogs. Thank you... 12/10 human used pet, it's super effective \n",
      "\n",
      " None => Blue  Tweet: RT @dog_rates: I usually only share these on Friday's, but this is Blue. He's a very smoochable pooch who needs your help. 13/10\n",
      "\n",
      "…\n",
      "\n",
      " not => None  Tweet: There's going to be a dog terminal at JFK Airport. This is not a drill. 10/10  \n",
      "\n",
      "\n",
      " None => Blue  Tweet: I usually only share these on Friday's, but this is Blue. He's a very smoochable pooch who needs your help. 13/10\n",
      "\n",
      " \n",
      "\n",
      " one => None  Tweet: Occasionally, we're sent fantastic stories. This is one of them. 14/10 for Grace \n",
      "\n",
      " None => Burke  Tweet: Here we have Burke (pupper) and Dexter (doggo). Pupper wants to be exactly like doggo. Both 12/10 would pet at same time \n",
      "\n",
      " incredibly => None  Tweet: We only rate dogs. Please stop sending in non-canines like this Freudian Poof Lion. This is incredibly frustrating... 11/10 \n",
      "\n",
      " None => Tickles  Tweet: RT @dog_rates: This a Norwegian Pewterschmidt named Tickles. Ears for days. 12/10 I care deeply for Tickles \n",
      "\n",
      " a => None  Tweet: Here is a perfect example of someone who has their priorities in order. 13/10 for both owner and Forrest \n",
      "\n",
      " mad => None  Tweet: RT @dog_rates: Say hello to mad pupper. You know what you did. 13/10 would pet until no longer furustrated \n",
      "\n",
      " an => None  Tweet: RT @dog_rates: This is an East African Chalupa Seal. We only rate dogs. Please only send in dogs. Thank you... 10/10 \n",
      "\n",
      " very => None  Tweet: RT @dog_rates: We only rate dogs. Pls stop sending in non-canines like this Mongolian grass snake. This is very frustrating. 11/10 https://…\n",
      "\n",
      " O => O'Malley  Tweet: This is O'Malley. That is how he sleeps. Doesn't care what you think about it. 10/10 comfy af \n",
      "\n",
      " a => None  Tweet: Guys this is getting so out of hand. We only rate dogs. This is a Galapagos Speed Panda. Pls only send dogs... 10/10 \n",
      "\n",
      " very => None  Tweet: We only rate dogs. Pls stop sending in non-canines like this Arctic Floof Kangaroo. This is very frustrating. 11/10 \n",
      "\n",
      " just => None  Tweet: RT @dog_rates: This is just downright precious af. 12/10 for both pupper and doggo \n",
      "\n",
      " None => Charley  Tweet: His name is Charley and he already has a new set of wheels thanks to donations. I heard his top speed was also increased. 13/10 for Charley\n",
      "\n",
      " my => Zoey  Tweet: This is my dog. Her name is Zoey. She knows I've been rating other dogs. She's not happy. 13/10 no bias at all \n",
      "\n",
      " one => None  Tweet: This is one of the most inspirational stories I've ever come across. I have no words. 14/10 for both doggo and owner \n",
      "\n",
      " not => None  Tweet: What jokester sent in a pic without a dog in it? This is not @rock_rates. This is @dog_rates. Thank you ...10/10 \n",
      "\n",
      " his => Quizno  Tweet: That is Quizno. This is his beach. He does not tolerate human shenanigans on his beach. 10/10 reclaim ur land doggo \n",
      "\n",
      " one => None  Tweet: This is one of the most reckless puppers I've ever seen. How she got a license in the first place is beyond me. 6/10 \n",
      "\n",
      " a => None  Tweet: This is a mighty rare blue-tailed hammer sherk. Human almost lost a limb trying to take these. Be careful guys. 8/10 \n",
      "\n",
      " a => None  Tweet: Viewer discretion is advised. This is a terrible attack in progress. Not even in water (tragic af). 4/10 bad sherk \n",
      "\n",
      " a => None  Tweet: This is a carrot. We only rate dogs. Please only send in dogs. You all really should know this by now ...11/10 \n",
      "\n",
      " an => None  Tweet: This is an Iraqi Speed Kangaroo. It is not a dog. Please only send in dogs. I'm very angry with all of you ...9/10 \n",
      "\n",
      " very => None  Tweet: We only rate dogs. Pls stop sending in non-canines like this Jamaican Flop Seal. This is very very frustrating. 9/10 \n",
      "\n",
      " actually => None  Tweet: This is actually a pupper and I'd pet it so well. 12/10\n",
      "\n",
      "\n",
      " a => None  Tweet: This is a very rare Great Alaskan Bush Pupper. Hard to stumble upon without spooking. 12/10 would pet passionately \n",
      "\n",
      " just => None  Tweet: This is just downright precious af. 12/10 for both pupper and doggo \n",
      "\n",
      " None => Bretagne  Tweet: After so many requests, this is Bretagne. She was the last surviving 9/11 search dog, and our second ever 14/10. RIP \n",
      "\n",
      " getting => None  Tweet: This is getting incredibly frustrating. This is a Mexican Golden Beaver. We only rate dogs. Only send dogs ...10/10 \n",
      "\n",
      " mad => None  Tweet: Say hello to mad pupper. You know what you did. 13/10 would pet until no longer furustrated \n",
      "\n",
      " very => None  Tweet: We only rate dogs. Please stop sending in non-canines like this Alaskan Flop Turtle. This is very frustrating. 10/10 \n",
      "\n",
      " this => None  Tweet: Say hello to this unbelievably well behaved squad of doggos. 204/170 would try to pet all at once \n",
      "\n",
      " unacceptable => None  Tweet: We only rate dogs. Pls stop sending non-canines like this Bulgarian Eyeless Porch Bear. This is unacceptable... 9/10 \n",
      "\n",
      " all => None  Tweet: This is all I want in my life. 12/10 for super sleepy pupper \n",
      "\n",
      " a => None  Tweet: People please. This is a Deadly Mediterranean Plop T-Rex. We only rate dogs. Only send in dogs. Thanks you... 11/10 \n",
      "\n",
      " old => None  Tweet: This is old now but it's absolutely heckin fantastic and I can't not share it with you all. 13/10  \n",
      "\n",
      " a => None  Tweet: This is a taco. We only rate dogs. Please only send in dogs. Dogs are what we rate. Not tacos. Thank you... 10/10 \n",
      "\n",
      " infuriating => None  Tweet: We 👏🏻 only 👏🏻 rate 👏🏻 dogs. Pls stop sending in non-canines like this Dutch Panda Worm. This is infuriating. 11/10 \n",
      "\n",
      " a => None  Tweet: Here is a heartbreaking scene of an incredible pupper being laid to rest. 10/10 RIP pupper \n",
      "\n",
      " a => None  Tweet: Here is a whole flock of puppers.  60/50 I'll take the lot \n",
      "\n",
      " a => None  Tweet: This is a Butternut Cumberfloof. It's not windy they just look like that. 11/10 back at it again with the red socks \n",
      "\n",
      " an => None  Tweet: This is an East African Chalupa Seal. We only rate dogs. Please only send in dogs. Thank you... 10/10 \n",
      "\n",
      " a => None  Tweet: This is a Wild Tuscan Poofwiggle. Careful not to startle. Rare tongue slip. One eye magical. 12/10 would def pet \n",
      "\n",
      " a => None  Tweet: \"Pupper is a present to world. Here is a bow for pupper.\" 12/10 precious as hell \n",
      "\n",
      " very => None  Tweet: We only rate dogs. Pls stop sending in non-canines like this Mongolian grass snake. This is very frustrating. 11/10 \n",
      "\n",
      " getting => None  Tweet: Please stop sending in saber-toothed tigers. This is getting ridiculous. We only rate dogs.\n",
      "...8/10 \n",
      "\n",
      " just => None  Tweet: This is just a beautiful pupper good shit evolution. 12/10 \n",
      "\n",
      " a => None  Tweet: This is a rare Arctic Wubberfloof. Unamused by the happenings. No longer has the appetites. 12/10 would totally hug \n",
      "\n",
      " the => None  Tweet: Stop sending in lobsters. This is the final warning. We only rate dogs. Thank you... 9/10 \n",
      "\n",
      " the => None  Tweet: This is the newly formed pupper a capella group. They're just starting out but I see tons of potential. 8/10 for all \n",
      "\n",
      " None => Thea  Tweet: We normally don't rate bears but this one seems nice. Her name is Thea. Appears rather fluffy. 10/10 good bear \n",
      "\n",
      " actually => None  Tweet: This is actually a lion. We only rate dogs. For the last time please only send dogs. Thank u.\n",
      "12/10 would still pet \n",
      "\n",
      " by => None  Tweet: This is by far the most coordinated series of pictures I was sent. Downright impressive in every way. 12/10 for all \n",
      "\n",
      " None => Sabertooth  Tweet: This pup's name is Sabertooth (parents must be cool). Ears for days. Jumps unannounced. 9/10 would pet diligently \n",
      "\n",
      " a => None  Tweet: Guys this really needs to stop. We've been over this way too many times. This is a giraffe. We only rate dogs.. 7/10 \n",
      "\n",
      " officially => None  Tweet: This is officially the greatest yawn of all time. 12/10 \n",
      "\n",
      " a => None  Tweet: This is a dog swinging. I really enjoyed it so I hope you all do as well. 11/10 \n",
      "\n",
      " the => None  Tweet: This is the happiest pupper I've ever seen. 10/10 would trade lives with \n",
      "\n",
      " the => None  Tweet: This is the saddest/sweetest/best picture I've been sent. 12/10 😢🐶 \n",
      "\n",
      " None => Yoshi  Tweet: &amp; this is Yoshi. Another world record contender 11/10 (what the hell is happening why are there so many contenders?) \n",
      "\n",
      " a => Wylie  Tweet: This is a Sizzlin Menorah spaniel from Brooklyn named Wylie. Lovable eyes. Chiller as hell. 10/10 and I'm out.. poof \n",
      "\n",
      " a => None  Tweet: Seriously guys?! Only send in dogs. I only rate dogs. This is a baby black bear... 11/10 \n",
      "\n",
      " a => None  Tweet: C'mon guys. We've been over this. We only rate dogs. This is a cow. Please only submit dogs. Thank you...... 9/10 \n",
      "\n",
      " a => None  Tweet: This is a fluffy albino Bacardi Columbia mix. Excellent at the tweets. 11/10 would hug gently \n",
      "\n",
      " life => None  Tweet: This is life-changing. 12/10 \n",
      "\n",
      " a => None  Tweet: This is a Sagitariot Baklava mix. Loves her new hat. 11/10 radiant pup \n",
      "\n",
      " one => None  Tweet: This is one esteemed pupper. Just graduated college. 10/10 what a champ \n",
      "\n",
      " a => None  Tweet: This is a heavily opinionated dog. Loves walls. Nobody knows how the hair works. Always ready for a kiss. 4/10 \n",
      "\n",
      " a => Kip  Tweet: This is a Lofted Aphrodisiac Terrier named Kip. Big fan of bed n breakfasts. Fits perfectly. 10/10 would pet firmly \n",
      "\n",
      " a => None  Tweet: This is a baby Rand Paul. Curls for days. 11/10 would cuddle the hell out of \n",
      "\n",
      " None => Judea  Tweet: Lots of pups here. All are Judea Hazelnuts. Exceptionally portable. 8/10 for all \n",
      "\n",
      " light => None  Tweet: This is light saber pup. Ready to fight off evil with light saber. 10/10 true hero \n",
      "\n",
      " just => None  Tweet: This is just impressive I have nothing else to say. 11/10 \n",
      "\n",
      " space => None  Tweet: This is space pup. He's very confused. Tries to moonwalk at one point. Super spiffy uniform. 13/10 I love space pup \n",
      "\n",
      " a => Jacob  Tweet: This is a Tuscaloosa Alcatraz named Jacob (Yacōb). Loves to sit in swing. Stellar tongue. 11/10 look at his feet \n",
      "\n",
      " the => None  Tweet: This is the best thing I've ever seen so spread it like wildfire &amp; maybe we'll find the genius who created it. 13/10 \n",
      "\n",
      " a => Rufus  Tweet: This is a Helvetica Listerine named Rufus. This time Rufus will be ready for the UPS guy. He'll never expect it 9/10 \n",
      "\n",
      " Amy => Queen  Tweet: This is Amy. She is Queen Starburst. 10/10 unexplainably juicy \n",
      "\n",
      " a => Spork  Tweet: This is a Deciduous Trimester mix named Spork. Only 1 ear works. No seat belt. Incredibly reckless. 9/10 still cute \n",
      "\n",
      " a => Cherokee  Tweet: This is a Rich Mahogany Seltzer named Cherokee. Just got destroyed by a snowball. Isn't very happy about it. 9/10 \n",
      "\n",
      " a => Hemry  Tweet: This is a Speckled Cauliflower Yosemite named Hemry. He's terrified of intruder dog. Not one bit comfortable. 9/10 \n",
      "\n",
      " a => Alphred  Tweet: This is a spotted Lipitor Rumpelstiltskin named Alphred. He can't wait for the Turkey. 10/10 would pet really well \n",
      "\n",
      " a => None  Tweet: This is a brave dog. Excellent free climber. Trying to get closer to God. Not very loyal though. Doesn't bark. 5/10 \n",
      "\n",
      " a => Alfredo  Tweet: This is a Coriander Baton Rouge named Alfredo. Loves to cuddle with smaller well-dressed dog. 10/10 would hug lots \n",
      "\n",
      " None => Zeus  Tweet: Here we have a Gingivitis Pumpernickel named Zeus. Unmatched tennis ball capacity. 10/10 would highly recommend \n",
      "\n",
      " None => Pancho  Tweet: Here we have Pancho and Peaches. Pancho is a Condoleezza Gryffindor, and Peaches is just an asshole. 10/10 &amp; 7/10 \n",
      "\n",
      " a => Leroi  Tweet: This is a Slovakian Helter Skelter Feta named Leroi. Likes to skip on roofs. Good traction. Much balance. 10/10 wow! \n",
      "\n",
      " a => None  Tweet: This is a wild Toblerone from Papua New Guinea. Mouth always open. Addicted to hay. Acts blind. 7/10 handsome dog \n",
      "\n",
      " an => Berta  Tweet: This is an Irish Rigatoni terrier named Berta. Completely made of rope. No eyes. Quite large. Loves to dance. 10/10 \n",
      "\n",
      " a => None  Tweet: Here is a horned dog. Much grace. Can jump over moons (dam!). Paws not soft. Bad at barking. 7/10 can still pet tho \n",
      "\n",
      " the => None  Tweet: Never forget this vine. You will not stop watching for at least 15 minutes. This is the second coveted.. 13/10 \n",
      "\n",
      " a => Chuk  Tweet: This is a Birmingham Quagmire named Chuk. Loves to relax and watch the game while sippin on that iced mocha. 10/10 \n",
      "\n",
      " a => None  Tweet: Here is a mother dog caring for her pups. Snazzy red mohawk. Doesn't wag tail. Pups look confused. Overall 4/10 \n",
      "\n",
      " None => Guss  Tweet: Here we have an Azerbaijani Buttermilk named Guss. He sees a demon baby Hitler behind his owner. 10/10 stays alert \n",
      "\n",
      " None => Bo  Tweet: These two dogs are Bo &amp; Smittens. Smittens is trying out a new deodorant and wanted Bo to smell it. 10/10 true pals \n",
      "\n",
      " a => Alfonso  Tweet: This is a Trans Siberian Kellogg named Alfonso. Huge ass eyeballs. Actually Dobby from Harry Potter. 7/10 \n",
      "\n",
      " None => Oliver  Tweet: This lil pup is Oliver. Hops around. Has wings but doesn't fly (lame). Annoying chirp. Won't catch tennis balls 2/10 \n",
      "\n",
      " a => Cheryl  Tweet: This is a Shotokon Macadamia mix named Cheryl. Sophisticated af. Looks like a disappointed librarian. Shh (lol) 9/10 \n",
      "\n",
      " a => Jessiga  Tweet: This is a rare Hungarian Pinot named Jessiga. She is either mid-stroke or got stuck in the washing machine. 8/10 \n",
      "\n",
      " a => Klint  Tweet: This is a southwest Coriander named Klint. Hat looks expensive. Still on house arrest :(\n",
      "9/10 \n",
      "\n",
      " None => Big  Tweet: Another topnotch dog. His name is Big Jumpy Rat. Massive ass feet. Superior tail. Jumps high af. 12/10 great pup \n",
      "\n",
      " None => Tickles  Tweet: This a Norwegian Pewterschmidt named Tickles. Ears for days. 12/10 I care deeply for Tickles \n",
      "\n",
      " a => Kohl  Tweet: This is a northern Wahoo named Kohl. He runs this town. Chases tumbleweeds. Draws gun wicked fast. 11/10 legendary \n",
      "\n",
      " a => Daryl  Tweet: This is a Dasani Kingfisher from Maine. His name is Daryl. Daryl doesn't like being swallowed by a panda. 8/10 \n",
      "\n",
      " a => Pepe  Tweet: This is a curly Ticonderoga named Pepe. No feet. Loves to jet ski. 11/10 would hug until forever \n",
      "\n",
      " None => Cupit  Tweet: These are Peruvian Feldspars. Their names are Cupit and Prencer. Both resemble Rand Paul. Sick outfits 10/10 &amp; 10/10 \n",
      "\n",
      " a => Octaviath  Tweet: This is a purebred Bacardi named Octaviath. Can shoot spaghetti out of mouth. 10/10 \n",
      "\n",
      " Lugan => Rocky  Tweet: This is Lugan. He is a Bohemian Rhapsody. Very confused dog. Thinks his name is Rocky. Not amused by the snows 10/10 \n",
      "\n",
      " a => Johm  Tweet: This is a golden Buckminsterfullerene named Johm. Drives trucks. Lumberjack (?). Enjoys wall. 8/10 would hug softly \n",
      "\n",
      " quite => None  Tweet: This is quite the dog. Gets really excited when not in water. Not very soft tho. Bad at fetch. Can't do tricks. 2/10 \n",
      "\n",
      " a => None  Tweet: This is a southern Vesuvius bumblegruff. Can drive a truck (wow). Made friends with 5 other nifty dogs (neat). 7/10 \n",
      "\n",
      " an => None  Tweet: This is an extremely rare horned Parthenon. Not amused. Wears shoes. Overall very nice. 9/10 would pet aggressively \n",
      "\n",
      " a => None  Tweet: This is a funny dog. Weird toes. Won't come down. Loves branch. Refuses to eat his food. Hard to cuddle with. 3/10 \n",
      "\n",
      " an => None  Tweet: This is an Albanian 3 1/2 legged  Episcopalian. Loves well-polished hardwood flooring. Penis on the collar. 9/10 \n",
      "\n",
      " the => None  Tweet: This is the happiest dog you will ever see. Very committed owner. Nice couch. 10/10 \n",
      "\n",
      " the => None  Tweet: Here is the Rand Paul of retrievers folks! He's probably good at poker. Can drink beer (lol rad). 8/10 good dog \n",
      "\n",
      " a => None  Tweet: My oh my. This is a rare blond Canadian terrier on wheels. Only $8.98. Rather docile. 9/10 very rare \n",
      "\n",
      " a => None  Tweet: Here is a Siberian heavily armored polar bear mix. Strong owner. 10/10 I would do unspeakable things to pet this dog \n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " an => None  Tweet: This is an odd dog. Hard on the outside but loving on the inside. Petting still fun. Doesn't play catch well. 2/10 \n",
      "\n",
      " a => None  Tweet: This is a truly beautiful English Wilson Staff retriever. Has a nice phone. Privileged. 10/10 would trade lives with \n",
      "\n",
      " a => None  Tweet: This is a purebred Piers Morgan. Loves to Netflix and chill. Always looks like he forgot to unplug the iron. 6/10 \n",
      "\n",
      " a => None  Tweet: Here is a very happy pup. Big fan of well-maintained decks. Just look at that tongue. 9/10 would cuddle af \n",
      "\n",
      " a => None  Tweet: This is a western brown Mitsubishi terrier. Upset about leaf. Actually 2 dogs here. 7/10 would walk the shit out of \n"
     ]
    }
   ],
   "source": [
    "for index, row in df_twitter_clean.query(\"new_name != name\")[['name' ,'new_name', 'text']].iterrows():\n",
    "    #print(\"---\\n\", type(row['name']), row['name'], '=>', row['new_name'], type(row['new_name']), row['text'])\n",
    "    print(\"\\n\", row['name'], '=>', row['new_name'], \" Tweet:\", row['text'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dog stages are often None. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "pupper     282\n",
       "doggo       94\n",
       "puppo       36\n",
       "floofer      9\n",
       "Name: new_stages, dtype: int64"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "None      2083\n",
       "pupper     256\n",
       "Name: pupper, dtype: int64"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "None     2243\n",
       "doggo      96\n",
       "Name: doggo, dtype: int64"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "None       2329\n",
       "floofer      10\n",
       "Name: floofer, dtype: int64"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "None     2309\n",
       "puppo      30\n",
       "Name: puppo, dtype: int64"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Regex to get the name, tryed on https://pythex.org/\n",
    "#name_pattern = re.compile(r\"(?:named| is|[mM]eet|have| are| hello to) [A-Z]'*[a-zA-Záàâäãåçéèêëíìîïñóòôöõúùûüýÿæœ]+\")\n",
    "# Method to get the name from a row\n",
    "def get_dog_stage(row):\n",
    "    names_to_find = ['pupper', 'doggo', 'floofer', 'puppo']\n",
    "    for to_find in names_to_find:\n",
    "        if row['text'].lower().find(to_find) != -1:\n",
    "            #print(to_find)\n",
    "            return to_find\n",
    "    return None\n",
    "\n",
    "df_twitter_clean['new_stages'] = df_twitter_all.apply(get_dog_stage, axis=1)\n",
    "df_twitter_clean['new_stages'].value_counts()\n",
    "\n",
    "df_twitter_clean['pupper'].value_counts()\n",
    "df_twitter_clean['doggo'].value_counts()\n",
    "df_twitter_clean['floofer'].value_counts()\n",
    "df_twitter_clean['puppo'].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We get more pupper and puppo, but less doggo and floofer. Why ?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "None!=pupper\n",
      "None!=doggo\n",
      "None!=puppo\n",
      "None!=puppo\n",
      "None!=puppo\n",
      "None!=puppo\n",
      "None!=doggo\n",
      "None!=puppo\n",
      "None!=puppo\n",
      "None!=doggo\n",
      "None!=doggo\n",
      "None!=doggo\n",
      "None!=pupper\n",
      "None!=puppo\n",
      "None!=doggo\n",
      "None!=doggo\n",
      "None!=doggo\n",
      "None!=doggo\n",
      "None!=pupper\n",
      "None!=pupper\n",
      "None!=pupper\n",
      "None!=doggo\n",
      "None!=pupper\n",
      "None!=pupper\n",
      "None!=pupper\n",
      "None!=pupper\n",
      "None!=pupper\n",
      "None!=pupper\n",
      "None!=pupper\n",
      "None!=pupper\n",
      "None!=pupper\n",
      "None!=pupper\n",
      "None!=pupper\n",
      "None!=pupper\n",
      "None!=pupper\n",
      "None!=pupper\n",
      "None!=pupper\n",
      "None!=pupper\n",
      "None!=pupper\n",
      "None!=pupper\n",
      "None!=pupper\n",
      "None!=pupper\n",
      "None!=pupper\n"
     ]
    }
   ],
   "source": [
    "def check_dog_stage(row):\n",
    "    # Test the case we found nothing and previous algo found it\n",
    "    if not row['new_stages']:\n",
    "        if row['pupper'] != 'None': print(\"pupper != None : \" + row['text'])\n",
    "        if row['doggo'] != 'None': print(\"doggo != None : \" + row['text'])\n",
    "        if row['puppo'] != 'None': print(\"puppo != None : \" + row['text'])\n",
    "        if row['floofer'] != 'None': print(\"flooter != None : \" + row['text'])\n",
    "    # Test the case we found something\n",
    "    if row['new_stages']:\n",
    "        if row['new_stages'] != row[str(row['new_stages'])]:\n",
    "            #print(row[str(row['new_stages'])] + \"!=\"+ row['new_stages'] + \" : \" + row['text'])\n",
    "            print(row[str(row['new_stages'])] + \"!=\"+ row['new_stages'])\n",
    "            return row['new_stages'] + \" : \" + row['text']\n",
    "    else:\n",
    "        return None\n",
    "\n",
    "_ = df_twitter_clean.apply(check_dog_stage, axis=1)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our extraction seems to work well. But there is another option : multiple values for the same raw.\n",
    "\n",
    "Let check it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ERROR : multiple values for: Here's a puppo participating in the #ScienceMarch. Cleverly disguising her own doggo agenda. 13/10 would keep the planet habitable for \n",
      "ERROR : multiple values for: At first I thought this was a shy doggo, but it's actually a Rare Canadian Floofer Owl. Amateurs would confuse the two. 11/10 only send dogs \n",
      "ERROR : multiple values for : This is Dido. She's playing the lead role in \"Pupper Stops to Catch Snow Before Resuming Shadow Box with Dried Apple.\" 13/10 (IG: didodoggo) \n",
      "ERROR : multiple values for : Here we have Burke (pupper) and Dexter (doggo). Pupper wants to be exactly like doggo. Both 12/10 would pet at same time \n",
      "ERROR : multiple values for : Like doggo, like pupper version 2. Both 11/10 \n",
      "ERROR : multiple values for : This is Bones. He's being haunted by another doggo of roughly the same size. 12/10 deep breaths pupper everything's fine \n",
      "ERROR : multiple values for : This is Pinot. He's a sophisticated doggo. You can tell by the hat. Also pointier than your average pupper. Still 10/10 would pet cautiously \n",
      "ERROR : multiple values for : Pupper butt 1, Doggo 0. Both 12/10 \n",
      "ERROR : multiple values for : RT @dog_rates: Like father (doggo), like son (pupper). Both 12/10 \n",
      "ERROR : multiple values for : RT @dog_rates: This is just downright precious af. 12/10 for both pupper and doggo \n",
      "ERROR : multiple values for : Meet Maggie &amp; Lila. Maggie is the doggo, Lila is the pupper. They are sisters. Both 12/10 would pet at the same time \n",
      "ERROR : multiple values for : Please stop sending it pictures that don't even have a doggo or pupper in them. Churlish af. 5/10 neat couch tho \n",
      "ERROR : multiple values for : This is just downright precious af. 12/10 for both pupper and doggo \n",
      "ERROR : multiple values for : Like father (doggo), like son (pupper). Both 12/10 \n"
     ]
    }
   ],
   "source": [
    "def merge_dog_stage(row):\n",
    "   if row['pupper'] != 'None':\n",
    "    if row['doggo'] != 'None' or row['puppo'] != 'None' or row['floofer'] != 'None':\n",
    "        print('ERROR : multiple values for :', row['text'])\n",
    "    return 'pupper'\n",
    "   elif row['doggo'] != 'None':\n",
    "        if row['pupper'] != 'None' or row['puppo'] != 'None' or row['floofer'] != 'None':\n",
    "            print('ERROR : multiple values for:', row['text'])\n",
    "        return 'doggo'\n",
    "   elif row['puppo'] != 'None':\n",
    "    if row['pupper'] != 'None' or row['pupper'] != 'None' or row['floofer'] != 'None':\n",
    "            print('ERROR : multiple values for :', row['text'])\n",
    "    return 'puppo'\n",
    "   elif row['floofer'] != 'None':\n",
    "        if row['pupper'] != 'None' or row['puppo'] != 'None' or row['pupper'] != 'None':\n",
    "            print('ERROR : multiple values for :', row['text'])\n",
    "        return 'flooter'\n",
    "   else:\n",
    "    return None\n",
    "\n",
    "_ = df_twitter_clean.apply(merge_dog_stage, axis=1)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Bingo!\n",
    "We will keep the values we found, even if they are wrong sometimes when there is more than one possibility."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## \"source\" include HTML tag\n",
    "\"source\" include HTML tag, but even without them, it do not seems of any use, I think there is not the values we expected in it. Here it's a case where we have to ask our client what he expect the data to be.\n",
    "I will clean HTML tag to keep only the title.\n",
    "\n",
    "Maybe it's from what device the tweet have been posted."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Twitter for iPhone     2205\n",
       "Vine - Make a Scene      91\n",
       "Twitter Web Client       33\n",
       "TweetDeck                10\n",
       "Name: new_source, dtype: int64"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "\n",
    "# Method to clean the source\n",
    "def clean_source(row):\n",
    "    return BeautifulSoup(row['source'], \"lxml\").text\n",
    "\n",
    "df_twitter_clean['new_source'] = df_twitter_all.apply(clean_source, axis=1)\n",
    "df_twitter_clean['new_source'].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " Well done. We could also have kept only the url."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Change object to date\n",
    "timestamp and retweeted_status_timestamp are \"object\" (i.e. string) so we will have to convert them to datetime. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    2017-08-01 16:23:56 +0000\n",
       "1    2017-08-01 00:17:27 +0000\n",
       "2    2017-07-31 00:18:03 +0000\n",
       "Name: timestamp, dtype: object"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_twitter_clean['timestamp'].head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0   2017-08-01 16:23:56\n",
       "1   2017-08-01 00:17:27\n",
       "2   2017-07-31 00:18:03\n",
       "Name: new_timestamp, dtype: datetime64[ns]"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_twitter_clean['new_timestamp'] =  pd.to_datetime(df_twitter_all['timestamp'])\n",
    "df_twitter_clean['new_timestamp'].head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Int64Index: 2339 entries, 0 to 2338\n",
      "Data columns (total 27 columns):\n",
      "tweet_id                          2339 non-null int64\n",
      "in_reply_to_status_id             78 non-null float64\n",
      "in_reply_to_user_id               78 non-null float64\n",
      "timestamp                         2339 non-null object\n",
      "source                            2339 non-null object\n",
      "retweeted_status_id               167 non-null float64\n",
      "retweeted_status_user_id          167 non-null float64\n",
      "retweeted_status_timestamp        167 non-null object\n",
      "expanded_urls                     2280 non-null object\n",
      "rating_numerator                  2339 non-null int64\n",
      "rating_denominator                2339 non-null int64\n",
      "name                              2339 non-null object\n",
      "doggo                             2339 non-null object\n",
      "floofer                           2339 non-null object\n",
      "pupper                            2339 non-null object\n",
      "puppo                             2339 non-null object\n",
      "favorite_count                    2339 non-null int64\n",
      "retweet_count                     2339 non-null int64\n",
      "text                              2339 non-null object\n",
      "new_rating_numerator              2339 non-null float64\n",
      "new_rating_denominator            2339 non-null float64\n",
      "new_name                          2339 non-null object\n",
      "new_stages                        421 non-null object\n",
      "new_source                        2339 non-null object\n",
      "new_timestamp                     2339 non-null datetime64[ns]\n",
      "new_retweeted_status_timestamp    167 non-null datetime64[ns]\n",
      "new_in_reply_to_status_id         2339 non-null int64\n",
      "dtypes: datetime64[ns](2), float64(6), int64(6), object(13)\n",
      "memory usage: 511.7+ KB\n"
     ]
    }
   ],
   "source": [
    "df_twitter_clean['new_retweeted_status_timestamp'] =  pd.to_datetime(df_twitter_all['retweeted_status_timestamp'])\n",
    "df_twitter_clean.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Wrong format on IDs\n",
    "in_reply_to_status_id, in_reply_to_user_id, retweeted_status_id and retweeted_status_user_id have been incorectly stored as Float64,  with values like 8.862663570751283e+17 or 2281181600.0\n",
    "\n",
    "Well, I made a mistake : Float64 is the wright format because it handle NaN for missing value, where int is not able to do it.\n",
    "\n",
    "So I will leave it like it. Below I demonstrate how to convert to int : we have to put 0 instead of NaN. It could also be done with \"fillna\".\n",
    "Or we could use string, but it is a waste of space to store number as string."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dtype('int64')"
      ]
     },
     "execution_count": 78,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import math\n",
    "import numpy as np\n",
    "if 'new_in_reply_to_status_id' in df_twitter_clean.columns:\n",
    "    hdf_twitter_clean.drop('new_in_reply_to_status_id', axis=1, inplace=True)\n",
    "\n",
    "\n",
    "# Method to get the name from a row\n",
    "def convert_to_int(value):\n",
    "    if math.isnan(value):\n",
    "        return 0\n",
    "    else:\n",
    "        #print(value)\n",
    "        return str(int(eval(str(value))))\n",
    "\n",
    "df_twitter_clean['new_in_reply_to_status_id'] = df_twitter_all.in_reply_to_status_id.apply(convert_to_int)\n",
    "df_twitter_clean['new_in_reply_to_status_id'] = df_twitter_clean.new_in_reply_to_status_id.astype(np.int64)\n",
    "df_twitter_clean['new_in_reply_to_status_id'].dtype"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Retweets has to be removed\n",
    "From the Twitter documentation (https://developer.twitter.com/en/docs/tweets/data-dictionary/overview/tweet-object.html) we learn that  if the represented Tweet is a reply, the field __in_reply_to_status_id__ will contain the \n",
    "integer representation of the original Tweet’s ID. \n",
    "\n",
    "And also that Retweets can be distinguished from typical Tweets by the existence of a retweeted_status attribute. \n",
    "\n",
    "So we just have to remove all the row where __retweeted_status_id__ and __in_reply_to_status_id__  is not NaN to remove reply and retweet.\n",
    "\n",
    "Removing may be done by drop, or we can create a \"copy\" of the dataset without the unwanted values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'df_twitter_clean' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-3-7a9baaf291c4>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m \u001b[0mdf_twitter_clean\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      3\u001b[0m \u001b[1;31m# Thanks to https://stackoverflow.com/questions/26535563/querying-for-nan-and-other-names-in-pandas\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[1;31m# I learn that NaN is not equal to NaN, so using var != var get all the line where var is NaN.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[0mdf_twitter_clean\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mdf_twitter_clean\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mquery\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"retweeted_status_id != retweeted_status_id and in_reply_to_status_id != in_reply_to_status_id\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'df_twitter_clean' is not defined"
     ]
    }
   ],
   "source": [
    "\n",
    "df_twitter_clean.shape\n",
    "# Thanks to https://stackoverflow.com/questions/26535563/querying-for-nan-and-other-names-in-pandas\n",
    "# I learn that NaN is not equal to NaN, so using var != var get all the line where var is NaN.\n",
    "df_twitter_clean = df_twitter_clean.query(\"retweeted_status_id != retweeted_status_id and in_reply_to_status_id != in_reply_to_status_id\")\n",
    "df_twitter_clean.shape\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Clean expanded_urls\n",
    "expanded_urls contains all url present in a tweet, in the original form, not the short coded tweeter form.\n",
    "More info in https://developer.twitter.com/en/docs/tweets/enrichments/overview/expanded-and-enhanced-urls.html\n",
    "\n",
    "So it may contain many comma separated url to the images or even website.\n",
    "\n",
    "We better have to put them in another table.\n",
    "\n",
    "And not the same as images predictions because it may not be images.\n",
    "\n",
    "We will have a table with tuple (tweet_id, url)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_url = XXX"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'df_twitter_clean' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-2-98c7fb07dad0>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mdf_twitter_clean\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mexpanded_urls\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mhead\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m3\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      2\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[1;31m# Method to get the url in expanded_urls\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[1;32mdef\u001b[0m \u001b[0mget_urls\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mvalue\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m     \u001b[0murls\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mvalue\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m','\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'df_twitter_clean' is not defined"
     ]
    }
   ],
   "source": [
    "df_twitter_clean.expanded_urls.head(3)\n",
    "\n",
    "# Method to get the url in expanded_urls\n",
    "def get_urls(value):\n",
    "    urls = value.split(',')\n",
    "    if len(urls) > 1:\n",
    "        print(urls)\n",
    "\n",
    "_ = df_twitter_all.expanded_urls.apply(get_urls)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Storing Data\n",
    "\n",
    "Store the clean DataFrame(s) in a CSV file with the main one named twitter_archive_master.csv. If additional files exist because multiple tables are required for tidiness, name these files appropriately. Additionally, you may store the cleaned data in a SQLite database (which is to be submitted as well if you do)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Before storing we drop the un-wanted columns\n",
    "'''\n",
    "\n",
    "'''Renaming columns'''\n",
    "\n",
    "df_twitter_clean.rename({'rating_denominator': 'bad_rating_denominator'}, axis=1, inplace=True)\n",
    "df_twitter_clean.rename({'rating_numerator': 'bad_rating_numerator'}, axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Remove retweet\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_twitter_all.to_csv('twitter_archive_master.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Store in SQLite\n",
    "# See https://pandas.pydata.org/pandas-docs/version/0.23.4/generated/pandas.DataFrame.to_sql.html\n",
    "\n",
    "from sqlalchemy import create_engine\n",
    "engine = create_engine('sqlite://', echo=False)\n",
    "df_twitter_all.to_sql('dogs_rate', con=engine)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check the content\n",
    "engine.execute(\"SELECT * FROM dogs_rate\").fetchall()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Analyzing, and Visualizing Data\n",
    "Analyze and visualize your wrangled data in your wrangle_act.ipynb Jupyter Notebook. At least\n",
    "- three (3) insights\n",
    "- one (1) visualization must be produced."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reporting for this Project\n",
    "\n",
    "Create a 300-600 word written report called wrangle_report.pdf or wrangle_report.html that briefly describes your wrangling efforts. This is to be framed as an internal document.\n",
    "\n",
    "Create a 250-word-minimum written report called act_report.pdf or act_report.html that communicates the insights and displays the visualization(s) produced from your wrangled data. This is to be framed as an external document, like a blog post or magazine article, for example.\n",
    "\n",
    "Both of these documents can be created in separate Jupyter Notebooks using the Markdown functionality of Jupyter Notebooks, then downloading those notebooks as PDF files or HTML files (see image below). You might prefer to use a word processor like Google Docs or Microsoft Word, however."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.5"
  },
  "widgets": {
   "state": {},
   "version": "1.1.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
