{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fitting Logistic Regression\n",
    "\n",
    "In this first notebook, you will be fitting a logistic regression model to a dataset where we would like to predict if a transaction is fraud or not.\n",
    "\n",
    "To get started let's read in the libraries and take a quick look at the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Put these at the top of every notebook, to get automatic reloading and inline plotting\n",
    "%reload_ext autoreload\n",
    "%autoreload 2\n",
    "%matplotlib inline\n",
    "from IPython.core.interactiveshell import InteractiveShell\n",
    "InteractiveShell.ast_node_interactivity = \"all\"\n",
    "def pv(variable):\n",
    "    if(len(variable)>1): print(variable, \":\", eval(variable))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>transaction_id</th>\n",
       "      <th>duration</th>\n",
       "      <th>day</th>\n",
       "      <th>fraud</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>28891</td>\n",
       "      <td>21.302600</td>\n",
       "      <td>weekend</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>61629</td>\n",
       "      <td>22.932765</td>\n",
       "      <td>weekend</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>53707</td>\n",
       "      <td>32.694992</td>\n",
       "      <td>weekday</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>47812</td>\n",
       "      <td>32.784252</td>\n",
       "      <td>weekend</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>43455</td>\n",
       "      <td>17.756828</td>\n",
       "      <td>weekend</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   transaction_id   duration      day  fraud\n",
       "0           28891  21.302600  weekend  False\n",
       "1           61629  22.932765  weekend  False\n",
       "2           53707  32.694992  weekday  False\n",
       "3           47812  32.784252  weekend  False\n",
       "4           43455  17.756828  weekend  False"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import statsmodels.api as sm\n",
    "\n",
    "\n",
    "df = pd.read_csv('./fraud_dataset.csv')\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0, 0, 1, ..., 1, 1, 0])"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.where(df[\"day\"].str.contains(\"weekday\"), 1, 0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`1.` As you can see, there are two columns that need to be changed to dummy variables.  Replace each of the current columns to the dummy version.  Use the 1 for `weekday` and `True`, and 0 otherwise.  Use the first quiz to answer a few questions about the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>transaction_id</th>\n",
       "      <th>duration</th>\n",
       "      <th>day</th>\n",
       "      <th>fraud</th>\n",
       "      <th>weekday</th>\n",
       "      <th>fraudulent</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>8791</th>\n",
       "      <td>63507</td>\n",
       "      <td>35.631321</td>\n",
       "      <td>weekday</td>\n",
       "      <td>False</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8792</th>\n",
       "      <td>74587</td>\n",
       "      <td>36.356404</td>\n",
       "      <td>weekend</td>\n",
       "      <td>False</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      transaction_id   duration      day  fraud  weekday  fraudulent\n",
       "8791           63507  35.631321  weekday  False        1           0\n",
       "8792           74587  36.356404  weekend  False        0           0"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "df['fraudulent'].mean() : 0.012168770612987604\n",
      "df.query(\"fraudulent == 1\").duration.mean() : 4.624247370615658\n",
      "df['weekday'].mean() : 0.3452746502900034\n",
      "df.query(\"fraudulent == 0\").duration.mean() : 30.013583132522584\n",
      "df.query(\"weekday == 1\").fraudulent.mean() : 0.026021080368906456\n",
      "df.query(\"weekday == 0\").fraudulent.mean() : 0.004863644259162758\n"
     ]
    }
   ],
   "source": [
    "# df_new = df.join(pd.get_dummies(df['day']))\n",
    "# df_new = df_new.join(pd.get_dummies(df['fraud']))\n",
    "\n",
    "df[\"weekday\"] = np.where(df[\"day\"].str.contains(\"weekday\"), 1, 0)\n",
    "df[\"fraudulent\"] = np.where( df[\"fraud\"], 1, 0 )\n",
    "df.tail(2)\n",
    "pv(\"df['fraudulent'].mean()\")\n",
    "pv('df.query(\"fraudulent == 1\").duration.mean()')\n",
    "pv(\"df['weekday'].mean()\")\n",
    "pv('df.query(\"fraudulent == 0\").duration.mean()')\n",
    "pv('df.query(\"weekday == 1\").fraudulent.mean()')\n",
    "pv('df.query(\"weekday == 0\").fraudulent.mean()')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`2.` Now that you have dummy variables, fit a logistic regression model to predict if a transaction is fraud using both day and duration.  Don't forget an intercept!  Use the second quiz below to assure you fit the model correctly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Optimization terminated successfully.\n",
      "         Current function value: 0.002411\n",
      "         Iterations 16\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<table class=\"simpletable\">\n",
       "<caption>Logit Regression Results</caption>\n",
       "<tr>\n",
       "  <th>Dep. Variable:</th>    <td>fraudulent</td>    <th>  No. Observations:  </th>   <td>  8793</td>  \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Model:</th>               <td>Logit</td>      <th>  Df Residuals:      </th>   <td>  8790</td>  \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Method:</th>               <td>MLE</td>       <th>  Df Model:          </th>   <td>     2</td>  \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Date:</th>          <td>Tue, 09 Apr 2019</td> <th>  Pseudo R-squ.:     </th>   <td>0.9633</td>  \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Time:</th>              <td>13:12:53</td>     <th>  Log-Likelihood:    </th>  <td> -21.200</td> \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>converged:</th>           <td>True</td>       <th>  LL-Null:           </th>  <td> -578.10</td> \n",
       "</tr>\n",
       "<tr>\n",
       "  <th> </th>                      <td> </td>        <th>  LLR p-value:       </th> <td>1.390e-242</td>\n",
       "</tr>\n",
       "</table>\n",
       "<table class=\"simpletable\">\n",
       "<tr>\n",
       "      <td></td>         <th>coef</th>     <th>std err</th>      <th>z</th>      <th>P>|z|</th>  <th>[0.025</th>    <th>0.975]</th>  \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>intercept</th> <td>    9.8709</td> <td>    1.944</td> <td>    5.078</td> <td> 0.000</td> <td>    6.061</td> <td>   13.681</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>duration</th>  <td>   -1.4637</td> <td>    0.290</td> <td>   -5.039</td> <td> 0.000</td> <td>   -2.033</td> <td>   -0.894</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>weekday</th>   <td>    2.5465</td> <td>    0.904</td> <td>    2.816</td> <td> 0.005</td> <td>    0.774</td> <td>    4.319</td>\n",
       "</tr>\n",
       "</table><br/><br/>Possibly complete quasi-separation: A fraction 0.98 of observations can be<br/>perfectly predicted. This might indicate that there is complete<br/>quasi-separation. In this case some parameters will not be identified."
      ],
      "text/plain": [
       "<class 'statsmodels.iolib.summary.Summary'>\n",
       "\"\"\"\n",
       "                           Logit Regression Results                           \n",
       "==============================================================================\n",
       "Dep. Variable:             fraudulent   No. Observations:                 8793\n",
       "Model:                          Logit   Df Residuals:                     8790\n",
       "Method:                           MLE   Df Model:                            2\n",
       "Date:                Tue, 09 Apr 2019   Pseudo R-squ.:                  0.9633\n",
       "Time:                        13:12:53   Log-Likelihood:                -21.200\n",
       "converged:                       True   LL-Null:                       -578.10\n",
       "                                        LLR p-value:                1.390e-242\n",
       "==============================================================================\n",
       "                 coef    std err          z      P>|z|      [0.025      0.975]\n",
       "------------------------------------------------------------------------------\n",
       "intercept      9.8709      1.944      5.078      0.000       6.061      13.681\n",
       "duration      -1.4637      0.290     -5.039      0.000      -2.033      -0.894\n",
       "weekday        2.5465      0.904      2.816      0.005       0.774       4.319\n",
       "==============================================================================\n",
       "\n",
       "Possibly complete quasi-separation: A fraction 0.98 of observations can be\n",
       "perfectly predicted. This might indicate that there is complete\n",
       "quasi-separation. In this case some parameters will not be identified.\n",
       "\"\"\""
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['intercept'] = 1\n",
    "mod = sm.Logit(df['fraudulent'], df[['intercept', 'duration', 'weekday']])\n",
    "res = mod.fit()\n",
    "res.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "np.exp(-1.4637) : 0.2313785882117941\n",
      "np.exp(2.5465) : 12.762357271496972\n"
     ]
    }
   ],
   "source": [
    "pv(\"np.exp(-1.4637)\")\n",
    "pv(\"np.exp(2.5465)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4.321921089278333"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "1/np.exp(-1.4637)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Interpreting Results of Logistic Regression\n",
    "\n",
    "In this notebook (and quizzes), you will be getting some practice with interpreting the coefficients in logistic regression.  Using what you saw in the previous video should be helpful in assisting with this notebook.\n",
    "\n",
    "The dataset contains four variables: `admit`, `gre`, `gpa`, and `prestige`:\n",
    "\n",
    "* `admit` is a binary variable. It indicates whether or not a candidate was admitted into UCLA (admit = 1) our not (admit = 0).\n",
    "* `gre` is the GRE score. GRE stands for Graduate Record Examination.\n",
    "* `gpa` stands for Grade Point Average.\n",
    "* `prestige` is the prestige of an applicant alta mater (the school attended before applying), with 1 being the highest (highest prestige) and 4 as the lowest (not prestigious).\n",
    "\n",
    "To start, let's read in the necessary libraries and data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>admit</th>\n",
       "      <th>gre</th>\n",
       "      <th>gpa</th>\n",
       "      <th>prestige</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>380</td>\n",
       "      <td>3.61</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>660</td>\n",
       "      <td>3.67</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   admit  gre   gpa  prestige\n",
       "0      0  380  3.61         3\n",
       "1      1  660  3.67         3"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv(\"./admissions.csv\")\n",
    "df.head(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are a few different ways you might choose to work with the `prestige` column in this dataset.  For this dataset, we will want to allow for the change from prestige 1 to prestige 2 to allow a different acceptance rate than changing from prestige 3 to prestige 4.\n",
    "\n",
    "1. With the above idea in place, create the dummy variables needed to change prestige to a categorical variable, rather than quantitative, then answer quiz 1 below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>admit</th>\n",
       "      <th>gre</th>\n",
       "      <th>gpa</th>\n",
       "      <th>prestige</th>\n",
       "      <th>prestige_3_4</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>380</td>\n",
       "      <td>3.61</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>660</td>\n",
       "      <td>3.67</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>800</td>\n",
       "      <td>4.00</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   admit  gre   gpa  prestige  prestige_3_4  1  2  3  4\n",
       "0      0  380  3.61         3             1  0  0  1  0\n",
       "1      1  660  3.67         3             1  0  0  1  0\n",
       "2      1  800  4.00         1             0  1  0  0  0"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df[\"prestige_3_4\"] = np.where(df[\"prestige\"] > 2, 1, 0)\n",
    "df[\"prestige_3_4\"] = np.where(df[\"prestige\"] > 2, 1, 0)\n",
    "df = df.join(pd.get_dummies(df['prestige']))\n",
    "df.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2    148\n",
       "3    121\n",
       "4     67\n",
       "1     61\n",
       "Name: prestige, dtype: int64"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df[\"prestige\"].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`2.` Now, fit a logistic regression model to predict if an individual is admitted using `gre`, `gpa`, and `prestige` with a baseline of the prestige value of `1`.  Use the results to answer quiz 2 and 3 below.  Don't forget an intercept."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Optimization terminated successfully.\n",
      "         Current function value: 0.573854\n",
      "         Iterations 6\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<table class=\"simpletable\">\n",
       "<caption>Logit Regression Results</caption>\n",
       "<tr>\n",
       "  <th>Dep. Variable:</th>       <td>admit</td>      <th>  No. Observations:  </th>  <td>   397</td>  \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Model:</th>               <td>Logit</td>      <th>  Df Residuals:      </th>  <td>   391</td>  \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Method:</th>               <td>MLE</td>       <th>  Df Model:          </th>  <td>     5</td>  \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Date:</th>          <td>Tue, 09 Apr 2019</td> <th>  Pseudo R-squ.:     </th>  <td>0.08166</td> \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Time:</th>              <td>13:12:53</td>     <th>  Log-Likelihood:    </th> <td> -227.82</td> \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>converged:</th>           <td>True</td>       <th>  LL-Null:           </th> <td> -248.08</td> \n",
       "</tr>\n",
       "<tr>\n",
       "  <th> </th>                      <td> </td>        <th>  LLR p-value:       </th> <td>1.176e-07</td>\n",
       "</tr>\n",
       "</table>\n",
       "<table class=\"simpletable\">\n",
       "<tr>\n",
       "      <td></td>         <th>coef</th>     <th>std err</th>      <th>z</th>      <th>P>|z|</th>  <th>[0.025</th>    <th>0.975]</th>  \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>intercept</th> <td>   -3.8769</td> <td>    1.142</td> <td>   -3.393</td> <td> 0.001</td> <td>   -6.116</td> <td>   -1.638</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>gre</th>       <td>    0.0022</td> <td>    0.001</td> <td>    2.028</td> <td> 0.043</td> <td> 7.44e-05</td> <td>    0.004</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>gpa</th>       <td>    0.7793</td> <td>    0.333</td> <td>    2.344</td> <td> 0.019</td> <td>    0.128</td> <td>    1.431</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>2</th>         <td>   -0.6801</td> <td>    0.317</td> <td>   -2.146</td> <td> 0.032</td> <td>   -1.301</td> <td>   -0.059</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>3</th>         <td>   -1.3387</td> <td>    0.345</td> <td>   -3.882</td> <td> 0.000</td> <td>   -2.015</td> <td>   -0.663</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>4</th>         <td>   -1.5534</td> <td>    0.417</td> <td>   -3.721</td> <td> 0.000</td> <td>   -2.372</td> <td>   -0.735</td>\n",
       "</tr>\n",
       "</table>"
      ],
      "text/plain": [
       "<class 'statsmodels.iolib.summary.Summary'>\n",
       "\"\"\"\n",
       "                           Logit Regression Results                           \n",
       "==============================================================================\n",
       "Dep. Variable:                  admit   No. Observations:                  397\n",
       "Model:                          Logit   Df Residuals:                      391\n",
       "Method:                           MLE   Df Model:                            5\n",
       "Date:                Tue, 09 Apr 2019   Pseudo R-squ.:                 0.08166\n",
       "Time:                        13:12:53   Log-Likelihood:                -227.82\n",
       "converged:                       True   LL-Null:                       -248.08\n",
       "                                        LLR p-value:                 1.176e-07\n",
       "==============================================================================\n",
       "                 coef    std err          z      P>|z|      [0.025      0.975]\n",
       "------------------------------------------------------------------------------\n",
       "intercept     -3.8769      1.142     -3.393      0.001      -6.116      -1.638\n",
       "gre            0.0022      0.001      2.028      0.043    7.44e-05       0.004\n",
       "gpa            0.7793      0.333      2.344      0.019       0.128       1.431\n",
       "2             -0.6801      0.317     -2.146      0.032      -1.301      -0.059\n",
       "3             -1.3387      0.345     -3.882      0.000      -2.015      -0.663\n",
       "4             -1.5534      0.417     -3.721      0.000      -2.372      -0.735\n",
       "==============================================================================\n",
       "\"\"\""
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['intercept'] = 1\n",
    "mod = sm.Logit(df['admit'], df[['intercept', 'gre', 'gpa', 2, 3, 4]])\n",
    "res = mod.fit()\n",
    "res.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GPA\n",
      "np.exp(0.779) : 2.179291883605348\n",
      "2\n",
      "np.exp(-0.68) : 0.5066169923655895\n",
      "1/np.exp(-0.68) : 1.973877732230448\n",
      "3\n",
      "np.exp(-1.3387) : 0.26218628930498067\n",
      "1/np.exp(-1.33387) : 3.795704376193652\n",
      "4\n",
      "np.exp(-1.553) : 0.2116121840667451\n",
      "1/np.exp(-1.553) : 4.725625815971862\n",
      "If an individual attended the most prestigious alma mater, they are __ more likely to be admitted than if they attended the second lowest in prestigious-ness, holding all other variables constant.\n",
      "(1/np.exp(-1.553)) - (1/np.exp(-0.68)) : 2.751748083741414\n",
      "If an individual attended the most prestigious alma mater, they are __ more likely to be admitted than if they attended the second most prestigious, holding all other variables constant.\n",
      "(1/np.exp(-1.553)) / np.exp(1.3387) : 1.238994297333484\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "intercept    0.020716\n",
       "gre          1.002221\n",
       "gpa          2.180027\n",
       "2            0.506548\n",
       "3            0.262192\n",
       "4            0.211525\n",
       "dtype: float64"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "intercept    48.272116\n",
       "gre           0.997784\n",
       "gpa           0.458710\n",
       "2             1.974147\n",
       "3             3.813995\n",
       "4             4.727566\n",
       "dtype: float64"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "prestige\n",
       "1    0.540984\n",
       "2    0.358108\n",
       "3    0.231405\n",
       "4    0.179104\n",
       "Name: admit, dtype: float64"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print('GPA')\n",
    "pv(\"np.exp(0.779)\")\n",
    "print(\"2\")\n",
    "pv(\"np.exp(-0.68)\")\n",
    "pv(\"1/np.exp(-0.68)\")\n",
    "print(\"3\")\n",
    "pv(\"np.exp(-1.3387)\")\n",
    "pv(\"1/np.exp(-1.33387)\")\n",
    "print(\"4\")\n",
    "pv(\"np.exp(-1.553)\")\n",
    "pv(\"1/np.exp(-1.553)\")\n",
    "print(\"If an individual attended the most prestigious alma mater, they are __ more likely to be admitted than if they attended the second lowest in prestigious-ness, holding all other variables constant.\")\n",
    "pv(\"(1/np.exp(-1.553)) - (1/np.exp(-0.68))\")\n",
    "print(\"If an individual attended the most prestigious alma mater, they are __ more likely to be admitted than if they attended the second most prestigious, holding all other variables constant.\")\n",
    "pv(\"(1/np.exp(-1.553)) / np.exp(1.3387)\")\n",
    "\n",
    "np.exp(res.params)\n",
    "1/_\n",
    "df.groupby('prestige').mean()['admit']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**That's right! Notice that in order to compare the lower prestigious values to the most prestigious (the baseline), we took one over the exponential of the coefficients. However, for a 1 unit increase, we could use the exponential directly.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model Diagnostics in Python\n",
    "\n",
    "In this notebook, you will be trying out some of the model diagnostics you saw from Sebastian, but in your case there will only be two cases - either admitted or not admitted.\n",
    "\n",
    "First let's read in the necessary libraries and the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import confusion_matrix, precision_score, recall_score, accuracy_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "np.random.seed(42)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`1.` Change prestige to dummy variable columns that are added to `df`.  Then divide your data into training and test data.  Create your test set as 20% of the data, and use a random state of 0.  Your response should be the `admit` column.  [Here](http://scikit-learn.org/stable/modules/generated/sklearn.model_selection.train_test_split.html) are the docs, which can also find with a quick google search if you get stuck."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>admit</th>\n",
       "      <th>gre</th>\n",
       "      <th>gpa</th>\n",
       "      <th>prestige</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>380</td>\n",
       "      <td>3.61</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>660</td>\n",
       "      <td>3.67</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>800</td>\n",
       "      <td>4.00</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>640</td>\n",
       "      <td>3.19</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>520</td>\n",
       "      <td>2.93</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   admit  gre   gpa  prestige\n",
       "0      0  380  3.61         3\n",
       "1      1  660  3.67         3\n",
       "2      1  800  4.00         1\n",
       "3      1  640  3.19         4\n",
       "4      0  520  2.93         4"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>admit</th>\n",
       "      <th>gre</th>\n",
       "      <th>gpa</th>\n",
       "      <th>prestige</th>\n",
       "      <th>prestige_1</th>\n",
       "      <th>prestige_2</th>\n",
       "      <th>prestige_3</th>\n",
       "      <th>prestige_4</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>380</td>\n",
       "      <td>3.61</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>660</td>\n",
       "      <td>3.67</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>800</td>\n",
       "      <td>4.00</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>640</td>\n",
       "      <td>3.19</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>520</td>\n",
       "      <td>2.93</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   admit  gre   gpa  prestige  prestige_1  prestige_2  prestige_3  prestige_4\n",
       "0      0  380  3.61         3           0           0           1           0\n",
       "1      1  660  3.67         3           0           0           1           0\n",
       "2      1  800  4.00         1           1           0           0           0\n",
       "3      1  640  3.19         4           0           0           0           1\n",
       "4      0  520  2.93         4           0           0           0           1"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv('./admissions.csv')\n",
    "df.head()\n",
    "df = df.join(pd.get_dummies(df['prestige'], prefix='prestige'))\n",
    "df.head()\n",
    "X = df[['gre', 'gpa', 'prestige_1', 'prestige_2', 'prestige_3']]\n",
    "y = df[['admit']]\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`2.` Now use [sklearn's Logistic Regression](http://scikit-learn.org/stable/modules/generated/sklearn.linear_model.LogisticRegression.html) to fit a logistic model using `gre`, `gpa`, and 3 of your `prestige` dummy variables.  For now, fit the logistic regression model without changing any of the hyperparameters.  \n",
    "\n",
    "The usual steps are:\n",
    "* Instantiate\n",
    "* Fit (on train)\n",
    "* Predict (on test)\n",
    "* Score (compare predict to test)\n",
    "\n",
    "As a first score, obtain the [confusion matrix](http://scikit-learn.org/stable/modules/generated/sklearn.metrics.confusion_matrix.html).  Then answer the first question below about how well your model performed on the test data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/media/data-nvme/dev/anaconda3/lib/python3.7/site-packages/sklearn/linear_model/logistic.py:433: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "/media/data-nvme/dev/anaconda3/lib/python3.7/site-packages/sklearn/utils/validation.py:761: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
       "          intercept_scaling=1, max_iter=100, multi_class='warn',\n",
       "          n_jobs=None, penalty='l2', random_state=None, solver='warn',\n",
       "          tol=0.0001, verbose=0, warm_start=False)"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = LogisticRegression()\n",
    "model.fit(X_train, y_train)\n",
    "y_pred = model.predict(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`3.` Now, try out a few additional metrics: [precision](http://scikit-learn.org/stable/modules/generated/sklearn.metrics.precision_score.html), [recall](http://scikit-learn.org/stable/modules/generated/sklearn.metrics.recall_score.html), and [accuracy](http://scikit-learn.org/stable/modules/generated/sklearn.metrics.accuracy_score.html) are all popular metrics, which you saw with Sebastian.  You could compute these directly from the confusion matrix, but you can also use these built in functions in sklearn.\n",
    "\n",
    "Another very popular set of metrics are [ROC curves and AUC](http://scikit-learn.org/stable/auto_examples/model_selection/plot_roc.html#sphx-glr-auto-examples-model-selection-plot-roc-py).  These actually use the probability from the logistic regression models, and not just the label.  [This](http://blog.yhat.com/posts/roc-curves.html) is also a great resource for understanding ROC curves and AUC.\n",
    "\n",
    "Try out these metrics to answer the second quiz question below.  I also provided the ROC plot below.  The ideal case is for this to shoot all the way to the upper left hand corner.  Again, these are discussed in more detail in the Machine Learning Udacity program."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "precision_score(y_test, y_pred) : 1.0\n",
      "recall_score(y_test, y_pred) : 0.08333333333333333\n",
      "accuracy_score(y_test, y_pred) : 0.725\n",
      "[[56  0]\n",
      " [22  2]]\n",
      "TN 56\n",
      "FP 0\n",
      "FN 22\n",
      "TP 2\n",
      "Precision :  1.0\n",
      "Recall :  0.08333333333333333\n",
      "F1 :  0.15384615384615385\n",
      "F2 :  0.10204081632653061\n",
      "FP : 0\n",
      "FN : 22\n",
      "TP : 2\n",
      "TN : 56\n",
      "TPR : 0.08333333333333333\n",
      "TNR : 1.0\n",
      "PPV : 1.0\n",
      "NPV : 0.717948717948718\n",
      "FPR : 0.0\n",
      "FNR : 0.9166666666666666\n",
      "FDR : 0.0\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# confusion_matrix, precision_score, recall_score, accuracy_score\n",
    "pv(\"precision_score(y_test, y_pred)\")\n",
    "pv(\"recall_score(y_test, y_pred)\")\n",
    "pv(\"accuracy_score(y_test, y_pred)\")\n",
    "\n",
    "# tn, fp, fn, tp = confusion_matrix([0, 1, 0, 1], [1, 1, 1, 0]).ravel()\n",
    "\n",
    "def compute_stats(y_test, y_pred, classes):\n",
    "    cm = confusion_matrix(y_test, y_pred)\n",
    "    print(cm)\n",
    "    tn, fp, fn, tp = cm.ravel()\n",
    "    print(\"TN\", tn)\n",
    "    print(\"FP\", fp)\n",
    "    print(\"FN\", fn)\n",
    "    print(\"TP\", tp)\n",
    "    #plot_confusion_matrix(y_test, y_pred, classes)\n",
    "    # precission = TP / (TP + FP)\n",
    "    precision = tp/(tp + fp);\n",
    "    print(\"Precision : \", precision)\n",
    "    # recall =  = TP / (TP + FN)\n",
    "    recall = tp / (tp+fn);\n",
    "    print(\"Recall : \", recall)\n",
    "    F1 = 2 * ((precision*recall)/(precision + recall))\n",
    "    print(\"F1 : \", F1)\n",
    "    F2 = 5 * ((precision*recall)/(4*precision + recall))\n",
    "    print(\"F2 : \", F2)\n",
    "\n",
    "    \n",
    "compute_stats(y_test, y_pred, [\"Admit\", \"Refused\"])\n",
    "\n",
    "\n",
    "cm = confusion_matrix(y_test, y_pred)\n",
    "\n",
    "# FP = cm.sum(axis=0) - np.diag(cm)\n",
    "# FN = cm.sum(axis=1) - np.diag(cm)\n",
    "# TP = np.diag(cm)\n",
    "# TN = cm.sum() - (FP + FN + TP)\n",
    "TN, FP, FN, TP = cm.ravel()\n",
    "pv(\"FP\")\n",
    "pv(\"FN\")\n",
    "pv(\"TP\")\n",
    "pv(\"TN\")\n",
    "\n",
    "\n",
    "# Sensitivity, hit rate, recall, or true positive rate\n",
    "TPR = TP/(TP+FN)\n",
    "# Specificity or true negative rate\n",
    "TNR = TN/(TN+FP) \n",
    "# Precision or positive predictive value\n",
    "PPV = TP/(TP+FP)\n",
    "# Negative predictive value\n",
    "NPV = TN/(TN+FN)\n",
    "# Fall out or false positive rate\n",
    "FPR = FP/(FP+TN)\n",
    "# False negative rate\n",
    "FNR = FN/(TP+FN)\n",
    "# False discovery rate\n",
    "FDR = FP/(TP+FP)\n",
    "\n",
    "# Overall accuracy\n",
    "ACC = (TP+TN)/(TP+FP+FN+TN)\n",
    "pv(\"TPR\")\n",
    "pv(\"TNR\")\n",
    "pv(\"PPV\")\n",
    "pv(\"NPV\")\n",
    "pv(\"FPR\")\n",
    "pv(\"FNR\")\n",
    "pv(\"FDR\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 792x576 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<ggplot: (-9223363285485891079)>"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "### Unless you install the ggplot library in the workspace, you will \n",
    "### get an error when running this code!\n",
    "#!pip uninstall ggplot\n",
    "from ggplot import *\n",
    "from sklearn.metrics import roc_curve, auc\n",
    "%matplotlib inline\n",
    "\n",
    "preds = model.predict_proba(X_test)[:,1]\n",
    "fpr, tpr, _ = roc_curve(y_test, preds)\n",
    "\n",
    "df = pd.DataFrame(dict(fpr=fpr, tpr=tpr))\n",
    "ggplot(df, aes(x='fpr', y='tpr')) +\\\n",
    "    geom_line() +\\\n",
    "    geom_abline(linetype='dashed')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**If we really care about correctly identifying the accepted students as accepted, which metric do we care about the most? => We want the FN to be as low as possible, so we want to maximize RECALL (near 1 as possible)** "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
